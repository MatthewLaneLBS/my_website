---
categories:  
- ""    #the front matter should be like the one found in, e.g., blog2.md. It cannot be like the normal Rmd we used
- ""
date: "2021-10-20"
description: Markdown output for 2nd group project # the title that will show up once someone gets to this page
draft: false
image: pic03.jpg # save picture in \static\img\blogs. Acceptable formats= jpg, jpeg, or png . Your iPhone pics wont work

keywords: ""
slug: group_project_2 # slug is the shorthand URL address... no spaces plz
title: Group Project 2
---



<div id="global-warming-and-political-views-gss" class="section level1">
<h1>Global warming and political views (GSS)</h1>
<p>We want to analyze whether there are any differences between the proportion of people who believe the earth is getting warmer and their political ideology. As usual, from the survey sample data, we will use the proportions to estimate values of <em>population parameters</em>. The file has 2253 observations on the following 2 variables:</p>
<ul>
<li><code>party_or_ideology</code>: a factor (categorical) variable with levels Conservative Republican, Liberal Democrat, Mod/Cons Democrat, Mod/Lib Republican</li>
<li><code>response</code> : whether the respondent believes the earth is warming or not, or Don’t know/ refuse to answer</li>
</ul>
<pre class="r"><code>global_warming_pew &lt;- read_csv(here::here(&quot;data&quot;, &quot;global_warming_pew.csv&quot;))</code></pre>
<p>We find that there are three possible responses in the data set :</p>
<ul>
<li>Earth is warming</li>
<li>Not warming</li>
<li>Don’t know / refuse to answer</li>
</ul>
<pre class="r"><code>global_warming_pew_long &lt;- global_warming_pew %&gt;% 
  count(party_or_ideology, response)

global_warming_pew_long</code></pre>
<pre><code>## # A tibble: 12 × 3
##    party_or_ideology       response                          n
##    &lt;chr&gt;                   &lt;chr&gt;                         &lt;int&gt;
##  1 Conservative Republican Don&#39;t know / refuse to answer    45
##  2 Conservative Republican Earth is warming                248
##  3 Conservative Republican Not warming                     450
##  4 Liberal Democrat        Don&#39;t know / refuse to answer    23
##  5 Liberal Democrat        Earth is warming                405
##  6 Liberal Democrat        Not warming                      23
##  7 Mod/Cons Democrat       Don&#39;t know / refuse to answer    45
##  8 Mod/Cons Democrat       Earth is warming                563
##  9 Mod/Cons Democrat       Not warming                     158
## 10 Mod/Lib Republican      Don&#39;t know / refuse to answer    23
## 11 Mod/Lib Republican      Earth is warming                135
## 12 Mod/Lib Republican      Not warming                     135</code></pre>
<p>We will be constructing three 95% confidence intervals to estimate population parameters, for the % who believe that <strong>Earth is warming</strong>, according to their party or ideology.</p>
<p>We transpose the data set to have responses per party, disregarding “Don’t know / refuse to answer”. We then create a 95% confidence intervals for the the percentage of people believing in Global Warming according to their political standpoint.</p>
<pre class="r"><code>global_warming_pew_wide &lt;- global_warming_pew_long %&gt;% 
  pivot_wider(names_from = response, # we pivot wide to have responses per party
              values_from = n) %&gt;% 
select(&quot;party_or_ideology&quot;, &quot;Earth is warming&quot;, &quot;Not warming&quot;) # we do not pick &quot;Don&#39;t know / refuse to answer&quot; responses

colnames(global_warming_pew_wide)[2:3] &lt;- c(&quot;warming&quot;, &quot;not_warming&quot;) 

global_warming_pew_wide &lt;- global_warming_pew_wide %&gt;%
  mutate(count = warming + not_warming, #calculate CI using formula by hand
         p_hat = warming/count,
         se = sqrt(p_hat * (1 - p_hat)/count),
         z_critical = qt(0.975, count - 1),
         lower = p_hat - z_critical * se,
         upper = p_hat + z_critical * se)

global_warming_pew_wide</code></pre>
<pre><code>## # A tibble: 4 × 9
##   party_or_ideolo… warming not_warming count p_hat     se z_critical lower upper
##   &lt;chr&gt;              &lt;int&gt;       &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 Conservative Re…     248         450   698 0.355 0.0181       1.96 0.320 0.391
## 2 Liberal Democrat     405          23   428 0.946 0.0109       1.97 0.925 0.968
## 3 Mod/Cons Democr…     563         158   721 0.781 0.0154       1.96 0.751 0.811
## 4 Mod/Lib Republi…     135         135   270 0.5   0.0304       1.97 0.440 0.560</code></pre>
<p>We include a visualisation of the final dataset which shows that party allegiance is an indicator of global warming recognition.</p>
<pre class="r"><code>global_warming_pew_wide %&gt;% 
  arrange(desc(p_hat)) %&gt;% 
  ggplot(aes(x = p_hat,
             y = fct_reorder(party_or_ideology, p_hat),
             color = party_or_ideology)) +
  geom_point() +
  
#Use geom_errorbarh to dipict the confidence interval  
  geom_errorbarh(aes(xmin = lower,
                 xmax = upper)) +
  scale_x_continuous(label = scales::percent) +
  labs(x = &quot;Probabilty of Answering Earth is warming&quot;,
       y = &quot;Party or Ideology&quot;) +
  theme_bw() +
  theme(text = element_text(size = 10),
        legend.position = &quot;bottom&quot;,
        legend.title = element_blank(),
        plot.margin = unit(c(5.5, 30, 5.5, 5.5), &quot;points&quot;))    #Specify margin to avoid cut-off of the legend</code></pre>
<p><img src="/blogs/group_project_2_files/figure-html/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>To make sure, let’s run <code>prop.test</code> to test if there are significant differences among four groups’ population mean.</p>
<pre class="r"><code>#Test for significant difference
prop.test(x = global_warming_pew_wide$warming,
          n = global_warming_pew_wide$count,
          conf.level = 0.95)</code></pre>
<pre><code>## 
##  4-sample test for equality of proportions without continuity
##  correction
## 
## data:  $ out of $global_warming_pew_wide out of global_warming_pew_widewarming out of count
## X-squared = 504, df = 3, p-value &lt;2e-16
## alternative hypothesis: two.sided
## sample estimates:
## prop 1 prop 2 prop 3 prop 4 
##  0.355  0.946  0.781  0.500</code></pre>
<p>The p-value is very small, indicating we can reject the null hypothesis of equality in proportions of the four groups’ population mean.</p>
<p>Given our samples and reading on <a href="https://www.brookings.edu/research/the-challenging-politics-of-climate-change/">The challenging politics of climate change</a>, we can interpret that there is a high level of correlation between party ideology and belief in global warming. The more we move to the centre left of the political landscape, the more it seems that people acknowledge the fact that the temperature on our planet is becoming warmer. This is generally a theme in US politics, as republican leaders tend to undermine the importance of climate change, with their voters following suit.</p>
<p>What we need to point out though, is the difference in the samples. Although we would expect liberal democrats to be more sensitive regarding global warming, it is a much smaller sample compared to the conservative republican and mod/cons democrats. The same issue goes even more obvious for mod/lib republican. Therefore, we have a clearer picture for conservative republicans and mod/cons democrats, where the first group definitely disregard global warming while the latter recognise it as an important problem.</p>
</div>
<div id="bidens-approval-margins" class="section level1">
<h1>Biden’s Approval Margins</h1>
<p>fivethirtyeight.com has detailed data on <a href="https://projects.fivethirtyeight.com/biden-approval-ratings">all polls that track the president’s approval</a>. We load data for Biden’s approval ratings modify it for dates to display correctly.</p>
<pre class="r"><code># Import approval polls data directly off fivethirtyeight website
approval_polllist &lt;- read_csv(&#39;https://projects.fivethirtyeight.com/biden-approval-data/approval_polllist.csv&#39;) 

# Use `lubridate` to fix dates, as they are given as characters.
approval_polllist &lt;- approval_polllist %&gt;%
  mutate(modeldate = mdy(modeldate),
         startdate = mdy(startdate),
         enddate = mdy(enddate),
         createddate = mdy(createddate),
         timestamp = hms(timestamp))

glimpse(approval_polllist)</code></pre>
<pre><code>## Rows: 1,970
## Columns: 22
## $ president           &lt;chr&gt; &quot;Joseph R. Biden Jr.&quot;, &quot;Joseph R. Biden Jr.&quot;, &quot;Jos…
## $ subgroup            &lt;chr&gt; &quot;All polls&quot;, &quot;All polls&quot;, &quot;All polls&quot;, &quot;All polls&quot;…
## $ modeldate           &lt;date&gt; 2021-10-21, 2021-10-21, 2021-10-21, 2021-10-21, 2…
## $ startdate           &lt;date&gt; 2021-01-19, 2021-01-19, 2021-01-20, 2021-01-20, 2…
## $ enddate             &lt;date&gt; 2021-01-21, 2021-01-21, 2021-01-21, 2021-01-22, 2…
## $ pollster            &lt;chr&gt; &quot;Rasmussen Reports/Pulse Opinion Research&quot;, &quot;Morni…
## $ grade               &lt;chr&gt; &quot;B&quot;, &quot;B&quot;, &quot;B-&quot;, &quot;B&quot;, &quot;B+&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B+&quot;, &quot;B…
## $ samplesize          &lt;dbl&gt; 1500, 15000, 1115, 15000, 1516, 1993, 15000, 1500,…
## $ population          &lt;chr&gt; &quot;lv&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;rv&quot;, &quot;a&quot;, &quot;lv&quot;, &quot;rv&quot;, &quot;…
## $ weight              &lt;dbl&gt; 0.3382, 0.2594, 1.1014, 0.2333, 1.2454, 0.0930, 0.…
## $ influence           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ approve             &lt;dbl&gt; 48, 50, 55, 51, 45, 56, 52, 48, 63, 58, 54, 48, 55…
## $ disapprove          &lt;dbl&gt; 45.0, 28.0, 32.0, 28.0, 28.0, 31.0, 29.0, 47.0, 37…
## $ adjusted_approve    &lt;dbl&gt; 50.4, 48.5, 53.8, 49.5, 46.4, 54.5, 50.5, 50.4, 58…
## $ adjusted_disapprove &lt;dbl&gt; 38.8, 31.2, 33.0, 31.2, 28.5, 34.2, 32.2, 40.8, 38…
## $ multiversions       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…
## $ tracking            &lt;lgl&gt; TRUE, TRUE, NA, TRUE, NA, NA, TRUE, TRUE, NA, NA, …
## $ url                 &lt;chr&gt; &quot;https://www.rasmussenreports.com/public_content/p…
## $ poll_id             &lt;dbl&gt; 74247, 74272, 74248, 74273, 74327, 74246, 74274, 7…
## $ question_id         &lt;dbl&gt; 139395, 139491, 139404, 139492, 139570, 139394, 13…
## $ createddate         &lt;date&gt; 2021-01-22, 2021-01-28, 2021-01-22, 2021-01-28, 2…
## $ timestamp           &lt;Period&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…</code></pre>
<div id="create-a-plot" class="section level2">
<h2>Create a plot</h2>
<p>As a next step, we calculate the average net approval rate (<code>approve</code> - <code>disapprove</code>) for each week since he got into office. We plot the net approval, along with its 95% confidence interval. There are various dates given for each poll, so we use <code>enddate</code>, i.e., the date the poll ended.</p>
<p>Our final plot should look similar to this:</p>
<p><img src="/Users/matthewtlane/Documents/LBS/CA09 Data Analytics for Finance/Lecture 1/Git Repo/my_website/images/biden_approval_margin.png" width="100%" style="display: block; margin: auto;" /></p>
<p>We produce a table called <code>weekly_approval</code> with Biden’s average net approval rating for every week he has been president. Then we calculate the 95% CIs. Before that, we check the differrent subgroups of polls in order to avoid duplicates.</p>
<pre class="r"><code>approval_polllist %&gt;% 
  count(subgroup)</code></pre>
<pre><code>## # A tibble: 3 × 2
##   subgroup      n
##   &lt;chr&gt;     &lt;int&gt;
## 1 Adults      564
## 2 All polls   935
## 3 Voters      471</code></pre>
<pre class="r"><code># Calculate the weekly net approval rate
weekly_approval &lt;- approval_polllist %&gt;%

#filter the data because there can be more than 1 results from the same poll and each result is under a different subgroup, we only choose the &quot;All polls&quot; subgroup because it has the most samples  
  filter(subgroup == &quot;All polls&quot;) %&gt;% 
  
#select only relevant data
  select(president, enddate, samplesize, approve, disapprove, poll_id, question_id) %&gt;% 
  
  mutate(net_approval = approve - disapprove, #calculate net approval
         week = week(enddate)) %&gt;%  #get week of the year (data is only for 2021)
  
#calculate weekly mean and confidence interval
  group_by(week) %&gt;% 
  summarise(mean_net_approval = mean(net_approval),
            sd_net_approval = sd(net_approval),
            count = n(),
            t_critical = qt(0.975, count - 1),
            se_net_approval = sd_net_approval/sqrt(count),
            margin_of_error = t_critical * se_net_approval,
            net_approval_low = mean_net_approval - margin_of_error,
            net_approval_high = mean_net_approval + margin_of_error)

head(weekly_approval)</code></pre>
<pre><code>## # A tibble: 6 × 9
##    week mean_net_approval sd_net_approval count t_critical se_net_approval
##   &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt;           &lt;dbl&gt;
## 1     3              18              8.89     5       2.78            3.97
## 2     4              18.3            9.44    23       2.07            1.97
## 3     5              16.6            7.70    25       2.06            1.54
## 4     6              16.7            8.37    18       2.11            1.97
## 5     7              16.4            7.57    24       2.07            1.54
## 6     8              14.9            7.86    24       2.07            1.60
## # … with 3 more variables: margin_of_error &lt;dbl&gt;, net_approval_low &lt;dbl&gt;,
## #   net_approval_high &lt;dbl&gt;</code></pre>
<p>We plot the net approvals as a next step and find a diminishing trend. This could probably be explained by the recent political disaster at Afghanistan and the fact that US needs to borrow more money to comply with its outstanding bond payments.</p>
<pre class="r"><code>#plot including week 7
ggplot(weekly_approval, aes(x = week, y = mean_net_approval)) +
  geom_point(color = &#39;orangered&#39;) +
  geom_smooth(se = F) +
  geom_line(color = &#39;orangered&#39;,
            size = 0.2,
            alpha = 0.6) +
  
#add a horizontal line for 0 (approval = disapproval)
  geom_line(aes(x = week, y = 0),
            color = &#39;orange&#39;,
            size = 2) +
  
#use geom_ribbon to depict confidence intervals
  geom_ribbon(aes(ymin = net_approval_low,
                  ymax = net_approval_high),
              fill = &#39;black&#39;,
              color = &#39;red&#39;,
              alpha = 0.1,
              size = 0.2) +
  
#change vertical gridlines
  scale_y_continuous(breaks = seq(-10, 20, by = 2.5),
                     minor_breaks = seq(-10, 20, 1.25)) +
  scale_x_continuous(breaks = seq(3, 39, by = 12),
                     minor_breaks = seq(3, 39, 6)) +
  
#some formatting
  ggplot2::annotate(&quot;text&quot;,
           x = 21, y = 30,
           label = &quot;2021&quot;,
           size = 4)+
  theme_bw() +
  theme(title = element_text(size = 12),
        panel.border = element_blank()) +
  coord_cartesian(ylim = c(-12, 30))+
  labs(title = &#39;Estimating Approval Margin (approve-disapprove) for Joe Biden&#39;,
       subtitle = &quot;Weekly average of all polls&quot;,
       x = &quot;Week of the year&quot;, 
       y = &quot;Average Approval Margin (Approve - Disapprove)&quot;) +
  NULL</code></pre>
<p><img src="/blogs/group_project_2_files/figure-html/unnamed-chunk-8-1.png" width="1152" style="display: block; margin: auto;" /></p>
</div>
<div id="compare-confidence-intervals" class="section level2">
<h2>Compare Confidence Intervals</h2>
<p>From the graph we can clearly see that the confidence interval for week 25 is considerably smaller than for week 3, indicated by the grey area around it and the orange borders. Looking at the <code>weekly_approval</code> table, we see that the boundaries of the confidence interval for week 25 are (9.79, 13.47), whereas it is (6.96, 29.03) for week 3. This difference can be explained by the sample size we have for the two weeks. For week 3, we only have 5 polls whereas for week 25 we have 29. Given that the formula to calculate the margin of error is <span class="math inline">\(t \frac{s}{\sqrt{n}}\)</span> where t is the critical t-value based on the t distribution that increases with decreasing degrees of freedom (<span class="math inline">\(n-1\)</span>). In addition, the standard error <span class="math inline">\(\frac{s}{\sqrt{n}}\)</span> is larger with a smaller <span class="math inline">\(n\)</span>. This means that the confidence interval will be smaller the higher our sample size, which is also what we see when we compare the intervals for week 3 and week 25.</p>
</div>
</div>
<div id="challenge-1-excess-rentals-in-tfl-bike-sharing" class="section level1">
<h1>Challenge 1: Excess rentals in TfL bike sharing</h1>
<p>We get the TfL latest data by running the following, and create a dataset on how many bikes were hired every day.</p>
<pre class="r"><code>url &lt;- &quot;https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx&quot;

# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp &lt;- tempfile(fileext = &quot;.xlsx&quot;)))</code></pre>
<pre><code>## Response [https://airdrive-secure.s3-eu-west-1.amazonaws.com/london/dataset/number-bicycle-hires/2021-09-23T12%3A52%3A20/tfl-daily-cycle-hires.xlsx?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAJJDIMAIVZJDICKHA%2F20211021%2Feu-west-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20211021T155925Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=71c3dc037fed840f44b1336f55cb1821353189094c682a7008b9184864668f58&amp;X-Amz-SignedHeaders=host]
##   Date: 2021-10-21 15:59
##   Status: 200
##   Content-Type: application/vnd.openxmlformats-officedocument.spreadsheetml.sheet
##   Size: 174 kB
## &lt;ON DISK&gt;  /var/folders/fm/f8st0r1s0yz8l0pws48sfs940000gp/T//RtmpnBL35l/file17e4923527f24.xlsx</code></pre>
<pre class="r"><code># Use read_excel to read it as dataframe
bike0 &lt;- read_excel(bike.temp,
                   sheet = &quot;Data&quot;,
                   range = cell_cols(&quot;A:B&quot;))

# change dates to get year, month, and week
bike &lt;- bike0 %&gt;% 
  clean_names() %&gt;% 
  rename (bikes_hired = number_of_bicycle_hires) %&gt;% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day))</code></pre>
<p>We can easily create a facet grid that plots bikes hired by month and year.</p>
<p><img src="/Users/matthewtlane/Documents/LBS/CA09 Data Analytics for Finance/Lecture 1/Git Repo/my_website/images/tfl_distributions_monthly.png" width="100%" style="display: block; margin: auto;" /></p>
<p>First let’s reproduce the graph above (and update it to the most recent dataset).</p>
<pre class="r"><code>#Create a density plot
bike %&gt;% 
  filter(year &gt;= 2015) %&gt;% 
  ggplot(aes(x = bikes_hired)) +
  geom_density() +
  
#Facet by year and month
  facet_grid(rows = vars(year),
             cols = vars(month)) +
  
#Set the x-lab into units of K(thousand)
  scale_x_continuous(label = scales::label_number(suffix = &quot;K&quot;,
                                                  scale = 0.001)) +
  
#Some formatting
  labs(x = &quot;Bike Rentals&quot;,
       y = NULL,
       title = &quot;Distribution of bikes hired per month&quot;)+
  theme_bw() +
  theme(text = element_text(size = 11),
        strip.background = element_rect(color = NA,
                                        fill = NA),
        panel.border = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.x = element_text(size = 7),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  NULL</code></pre>
<p><img src="/blogs/group_project_2_files/figure-html/unnamed-chunk-9-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>In May and June 2020, the distributions of daily bike rentals are visually flatter, and more uniformly distributed, than other months. Compared to previous and preceding months, May and June have several observations of low volumes of bikes being hired, likely due to the Covid-19 stay at home orders being ordered by the government, however these restrictions were phased out in June with non-essential shops and schools reopening. We believe this is reflected in the data by the more consistent distribution of daily rental volume than compared to other months.</p>
<p>For instance, during restricted periods we expect daily bike rental figures were low between 20-40k due to sociable/voluntary rental being banned and only key workers or those who cannot work from home using bikes to commute - other means of public transport were restricted or banned. However, once restrictions were relaxed we expect large numbers of bikes were rented by households who wanted to enjoy their new found freedoms outside of their homes. Consequently, because the rental figures were split somewhat equally across the distribution and not in a concentrated manner we see little positive kurtosis or concentration in the distribution creating a somewhat flat distribution different to other months.</p>
<p>Simply, the distribution caused to daily life has meant there is a wider spread of daily bike rentals over the two months and so neither possesses a histogram distribution similar to those of other months.</p>
<p>To better understand the excess rental phenomena, we will reproduce the two graphs below, one on monthly change and the other on weekly change from the expected level of monthly or weekly rentals. The two grey shaded rectangles in the second graph correspond to Q2 (weeks 14-26) and Q4 (weeks 40-52).</p>
<p><img src="/Users/matthewtlane/Documents/LBS/CA09 Data Analytics for Finance/Lecture 1/Git Repo/my_website/images/tfl_monthly.png" width="100%" style="display: block; margin: auto;" /></p>
<p><img src="/Users/matthewtlane/Documents/LBS/CA09 Data Analytics for Finance/Lecture 1/Git Repo/my_website/images/tfl_weekly.png" width="100%" style="display: block; margin: auto;" /></p>
<p>The expected number of rentals per week or month is calculated from data between 2016-2019 and we will see how each week or month of 2020-2021 compares to the expected rentals. We think of the calculation <code>excess_rentals = actual_rentals - expected_rentals</code>.</p>
<p>To decide whether we use mean or median to represent expected level of bike rentals, let’s first graph a box plot to see how daily data are distributed.</p>
<pre class="r"><code>#Create a boxplot to see the pattern of distribution in 2016-2019
bike %&gt;% 
  filter(year &gt;= 2016, year &lt;= 2019) %&gt;% 
  ggplot(aes(y = bikes_hired)) +
  facet_wrap(~year,
             nrow = 2) +
  geom_boxplot() +
  NULL</code></pre>
<p><img src="/blogs/group_project_2_files/figure-html/unnamed-chunk-10-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>We can see that the distributions of daily rentals in 2016-2019 are not very skewed and there are few outliers in the dataset. In this case, considering mean has a better statistical implication than median, we decide to use monthly or weekly mean as our expected levels of bike rentals.</p>
<p>We will first play with our daily data to make a monthly summary of bike rentals and make some transformation for the purpose of further plotting.</p>
<pre class="r"><code>#Summarize mean of rentals by month
bike_monthly &lt;- bike %&gt;% 
  filter(year &gt;= 2016) %&gt;% 
  group_by(year, month) %&gt;% 
  summarize(bikes_hired_monthly_mean = mean(bikes_hired))

#Calculate the expected monthly rentals, i.e. the mean of monthly mean in 2016-2019
bike_monthly_expected &lt;- bike_monthly %&gt;% 
  filter(year &lt;= 2019) %&gt;% 
  group_by(month) %&gt;% 
  summarize(bikes_hired_monthly_expected = mean(bikes_hired_monthly_mean))

#Incorporate the expected monthly rentals to bike_monthly
bike_monthly &lt;- left_join(bike_monthly,
                          bike_monthly_expected, 
                          by = &quot;month&quot;)

#Create two new columns for excess(+/-) rentals per month compared with expected levels
bike_monthly &lt;- bike_monthly %&gt;% 
  mutate(bikes_hired_monthly_excess = pmax(bikes_hired_monthly_mean - bikes_hired_monthly_expected, 0),
         bikes_hired_monthly_shortage = pmax(bikes_hired_monthly_expected - bikes_hired_monthly_mean, 0))

head(bike_monthly)</code></pre>
<pre><code>## # A tibble: 6 × 6
## # Groups:   year [1]
##    year month bikes_hired_monthly_mean bikes_hired_monthly_… bikes_hired_monthl…
##   &lt;dbl&gt; &lt;ord&gt;                    &lt;dbl&gt;                 &lt;dbl&gt;               &lt;dbl&gt;
## 1  2016 Jan                     18914.                20617.                   0
## 2  2016 Feb                     20608.                22062.                   0
## 3  2016 Mar                     21435                 23237.                   0
## 4  2016 Apr                     25444.                28299.                   0
## 5  2016 May                     32699.                33270.                   0
## 6  2016 Jun                     32108.                35413.                   0
## # … with 1 more variable: bikes_hired_monthly_shortage &lt;dbl&gt;</code></pre>
<p>Now we can reproduce the first graph.</p>
<pre class="r"><code>#Lines for actual monthly rentals and expected monthly rentals
ggplot(bike_monthly, 
       aes(x = month)) +
  geom_line(aes(y = bikes_hired_monthly_mean,
                group = 1)) +
  geom_line(aes(y = bikes_hired_monthly_expected,
                group = 1),
            size = 1.5,
            color = &quot;blue&quot;) +
  facet_wrap(~ year) +
  
#Use geom_ribbon for excess and shortage compared with expected levels
  geom_ribbon(aes(ymin = bikes_hired_monthly_expected,
                  ymax = bikes_hired_monthly_expected + bikes_hired_monthly_excess,
                  group = 1),
              fill = &quot;green4&quot;,
              alpha = 0.25) +
  geom_ribbon(aes(ymin = bikes_hired_monthly_expected - bikes_hired_monthly_shortage,
                  ymax = bikes_hired_monthly_expected,
                  group = 1),
              fill = &quot;red3&quot;,
              alpha = 0.25) +
  
#Some formatting
  scale_y_continuous(label = scales::comma,
                     limits = c(13000, NA)) +
  labs(x = &quot;Month&quot;,
       y = &quot;Bike Rentals&quot;,
       title = &quot;Monthly changes in TfL bike rentals&quot;,
       subtitle = &quot;Change from monthly average shown in blue \nand calculated between 2016-2019&quot;,
       caption = &quot;Source: TfL, London Data Store&quot;) +
  theme_bw() +
  theme(text = element_text(size = 14),
        strip.background = element_rect(color = NA,
                                        fill = NA),
        panel.border = element_blank()) +
  NULL</code></pre>
<p><img src="/blogs/group_project_2_files/figure-html/unnamed-chunk-12-1.png" width="1440" style="display: block; margin: auto;" /></p>
<p>The graph above depicts expected and real monthly changes in bike rentals across London. The blue line shows the expected daily number of bikes rented for a given month and the black line represents the actual number. The green or red fill shows the excess or under-use of bikes for that month compared to the expected figure.</p>
<ul>
<li><p>The graph shows that for 2016, 2017 and 2019 actual usage is approximately in-line with expectation with an equal distribution of months marginally above and below expectation.</p></li>
<li><p>For 2018, June and July are considerable above expectation, however when compared to weather reports for those months this is perhaps expected. The summer of 2018 was the joint average hottest ever recorded in the UK with a peak temperature of 35.3 celsius, due to this a large excess of rentals is justified and in line with expectation.</p></li>
<li><p>As expected, 2020 and 2021 deviate from expectation considerably several times, almost certainly due to the Covid-19 pandemic. In March and April 2020, actual rental figures were well below expectation, likely because of the stay at home order issued by the UK government during these months. Then for May and June the daily rental average jumped sharply from &lt;20k to &gt;35k, this time likely due to the relaxation of some restrictions as well as key workers and those who couldn’t work from home commuting using bikes as other public transport provision was reduced or canceled. September 2020 was also well above expectation, likely due to the imposition of the “Rule of 6” for indoor and outdoor gatherings as well as behavioural changes in society - people looking to spend more time outside of their homes and outdoors in general having been forced to remain at home for many weeks. The rest of 2020 was fairly in line with expectation despite a circuit-breaker lockdown but this could be because the expectation of daily rentals dropped sharply already.</p></li>
<li><p>So far in 2021, only January and February have been well below expectation. This is likely due to the third national lockdown imposed by the government forcing households to remain at home. Since the relaxing of restrictions average daily rentals have remained somewhat in line with expectation.</p></li>
</ul>
<p>We then do the same with weekly data but calculate excess(+/-) rentals as percent of expected weekly rentals.</p>
<p>Before building the dataset, we first need to check if our previous steps have correctly categorized data for week 1, 52 and 53. It is highly possible that the first few days of a year is labelled week 52 or 53 for the previous year, or the last few days of a year is labelled week 1 for the next year. Obviously you can’t have 10 days in your week 52 data.</p>
<pre class="r"><code>#Display all week 1, 52 and 53 data
bike %&gt;%
  filter(year &gt;= 2016,
         week %in% c(1, 52, 53) )</code></pre>
<pre><code>## # A tibble: 87 × 5
##    day                 bikes_hired  year month  week
##    &lt;dttm&gt;                    &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;dbl&gt;
##  1 2016-01-01 00:00:00        9922  2016 Jan      53
##  2 2016-01-02 00:00:00        7246  2016 Jan      53
##  3 2016-01-03 00:00:00        4894  2016 Jan      53
##  4 2016-01-04 00:00:00       20644  2016 Jan       1
##  5 2016-01-05 00:00:00       22934  2016 Jan       1
##  6 2016-01-06 00:00:00       23199  2016 Jan       1
##  7 2016-01-07 00:00:00       18225  2016 Jan       1
##  8 2016-01-08 00:00:00       20948  2016 Jan       1
##  9 2016-01-09 00:00:00       11674  2016 Jan       1
## 10 2016-01-10 00:00:00       14447  2016 Jan       1
## # … with 77 more rows</code></pre>
<p>Our concern is correct: the first 3 days of 2016 are categorized into week 53, but it’s the week 53 of year 2015 instead of 2016. The same issue also occurs in all years in our current dataset. To rectify this, we need to change any mislabeled days in week 1, 52 or 53 and put them into the correct year.</p>
<pre class="r"><code>#Create a new dataset to aviod overwriting
bike_adjust &lt;- bike

#For all week 52 and 53 data in January, move their year label to the previous year
bike_adjust$year &lt;- if_else(bike_adjust$week &gt;= 52 &amp; bike_adjust$month == &quot;Jan&quot;,
                            bike_adjust$year - 1,
                            bike_adjust$year)

#For all week 1 data in December, move their year label to the next year
bike_adjust$year &lt;- if_else(bike_adjust$week == 1 &amp; bike_adjust$month == &quot;Dec&quot;,
                            bike_adjust$year + 1,
                            bike_adjust$year)

#Display all week 1, 52 and 53 data
bike_adjust %&gt;%
  filter(year &gt;= 2016,
         week %in% c(1, 52, 53) )</code></pre>
<pre><code>## # A tibble: 84 × 5
##    day                 bikes_hired  year month  week
##    &lt;dttm&gt;                    &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;dbl&gt;
##  1 2016-01-04 00:00:00       20644  2016 Jan       1
##  2 2016-01-05 00:00:00       22934  2016 Jan       1
##  3 2016-01-06 00:00:00       23199  2016 Jan       1
##  4 2016-01-07 00:00:00       18225  2016 Jan       1
##  5 2016-01-08 00:00:00       20948  2016 Jan       1
##  6 2016-01-09 00:00:00       11674  2016 Jan       1
##  7 2016-01-10 00:00:00       14447  2016 Jan       1
##  8 2016-12-26 00:00:00       11637  2016 Dec      52
##  9 2016-12-27 00:00:00       11003  2016 Dec      52
## 10 2016-12-28 00:00:00       12530  2016 Dec      52
## # … with 74 more rows</code></pre>
<p>We have now taken care of the mislabeling issue. To make sure of this, let’s look at how many days are in each week of each year.</p>
<pre class="r"><code>#Count how many days are in each week and order the table by the count
bike_adjust %&gt;% 
  filter(year &gt;= 2016) %&gt;% 
  group_by(year, week) %&gt;% 
  summarize(number_of_days = count(week)) %&gt;% 
  arrange(number_of_days)</code></pre>
<pre><code>## # A tibble: 296 × 3
## # Groups:   year [6]
##     year  week number_of_days
##    &lt;dbl&gt; &lt;dbl&gt;          &lt;int&gt;
##  1  2021    35              2
##  2  2016     1              7
##  3  2016     2              7
##  4  2016     3              7
##  5  2016     4              7
##  6  2016     5              7
##  7  2016     6              7
##  8  2016     7              7
##  9  2016     8              7
## 10  2016     9              7
## # … with 286 more rows</code></pre>
<p>Except for the week 35 of year 2021, which is our most recent data, all weeks in our <code>bike_adjust</code> dataset have 7 days. We can now proceed with further processing and visualization.</p>
<p>Note that we only have one sample of week 53 in our current dataset, and it is in year 2020. We don’t have any week 53 in 2016-2019 (except for the first 3 days of 2016, which we have already reclassified into year 2015), and therefore there is no expected weekly rentals for week 53. To address this anamoly, we just discard the week 53 data from our new dataset to avoid an <code>NA</code> value.</p>
<pre class="r"><code>#Summarize mean of rentals by week
bike_weekly &lt;- bike_adjust %&gt;% 
  filter(year &gt;= 2016, week != 53) %&gt;% 
  group_by(year, week) %&gt;% 
  summarize(bikes_hired_weekly_mean = mean(bikes_hired))

#Calculate the expected weekly rentals, i.e. the mean of weekly mean in 2016-2019
bike_weekly_expected &lt;- bike_weekly %&gt;% 
  filter(year &lt;= 2019) %&gt;% 
  group_by(week) %&gt;% 
  summarize(bikes_hired_weekly_expected = mean(bikes_hired_weekly_mean))

#Incorporate the expected weekly rentals to bike_weekly
bike_weekly &lt;- left_join(bike_weekly,
                         bike_weekly_expected,
                         by = &quot;week&quot;)

#Create two new columns for percentage excess(+/-) rentals per week compared with expected levels
bike_weekly &lt;- bike_weekly %&gt;% 
  mutate(bikes_hired_weekly_diff = (bikes_hired_weekly_mean - bikes_hired_weekly_expected)/bikes_hired_weekly_expected,
         bikes_hired_weekly_excess = pmax(bikes_hired_weekly_diff, 0),
         bikes_hired_weekly_shortage = pmin(bikes_hired_weekly_diff, 0))

head(bike_weekly)</code></pre>
<pre><code>## # A tibble: 6 × 7
## # Groups:   year [1]
##    year  week bikes_hired_weekly_mean bikes_hired_weekly_ex… bikes_hired_weekly…
##   &lt;dbl&gt; &lt;dbl&gt;                   &lt;dbl&gt;                  &lt;dbl&gt;               &lt;dbl&gt;
## 1  2016     1                  18867.                 17388.             0.0851 
## 2  2016     2                  20255.                 22056.            -0.0817 
## 3  2016     3                  20937.                 21892.            -0.0436 
## 4  2016     4                  20550.                 21470.            -0.0428 
## 5  2016     5                  21230                  21194.             0.00169
## 6  2016     6                  19896.                 20226.            -0.0163 
## # … with 2 more variables: bikes_hired_weekly_excess &lt;dbl&gt;,
## #   bikes_hired_weekly_shortage &lt;dbl&gt;</code></pre>
<p>Now we can reproduce the second graph.</p>
<pre class="r"><code>#Line for the percentage change compared with expected weekly bike rentals
ggplot(bike_weekly, aes(x = week)) +
  geom_line(aes(y = bikes_hired_weekly_diff)) +
  facet_wrap(~ year) +

#Use geom_ribbon for percentage excess and shortage compared with expected levels   
  geom_ribbon(aes(ymin = 0,
                  ymax = bikes_hired_weekly_excess),
              fill = &quot;green4&quot;,
              alpha = 0.25) +
  geom_ribbon(aes(ymin = bikes_hired_weekly_shortage,
                  ymax = 0),
              fill = &quot;red3&quot;,
              alpha = 0.25) +
  
#Create the two grey shaded rectangles for Q2 and Q4
#geom_rect doesn&#39;t produce the desired effect in terms of alpha, so we use annotate(geom = &quot;rect&quot;, ...) to optimize the result
  ggplot2::annotate(geom = &quot;rect&quot;,
           xmin = 13, xmax = 26,
           ymin = -Inf, ymax = Inf,
           fill = &quot;grey&quot;,
           alpha = 0.25) +
  ggplot2::annotate(geom = &quot;rect&quot;,
           xmin = 39, xmax = 53,
           ymin = -Inf, ymax = Inf,
           fill = &quot;grey&quot;,
           alpha = 0.25) +
  
#Use geom_rug for the small indicators at the bottom, with two differently sliced dataset from bike_weekly
  geom_rug(data = bike_weekly[bike_weekly[ , &quot;bikes_hired_weekly_excess&quot;] != 0, ,
                              drop = FALSE],
           color = &quot;green4&quot;,
           sides = &quot;b&quot;) +
  geom_rug(data = bike_weekly[bike_weekly[ , &quot;bikes_hired_weekly_shortage&quot;] != 0, ,
                              drop = FALSE],
           color = &quot;red3&quot;,
           sides = &quot;b&quot;) +
  
#Some formatting
  scale_x_continuous(breaks = c(13, 26, 39, 52)) +
  scale_y_continuous(label = scales::percent) +
  labs(x = &quot;week&quot;,
       y = NULL,
       title = &quot;Weekly changes in TfL bike rentals&quot;,
       subtitle = &quot;% change from weekly averages \ncalculated between 2016-2019&quot;,
       caption = &quot;Source: TfL, London Data Store&quot;) +
  theme_bw() +
  theme(text = element_text(size = 14),
        strip.background = element_rect(color = NA,
                                        fill = NA),
        panel.border = element_blank()) +
  NULL</code></pre>
<p><img src="/blogs/group_project_2_files/figure-html/unnamed-chunk-17-1.png" width="1440" style="display: block; margin: auto;" /></p>
<p>The graph above maps weekly changes in bike rentals across London from 2016 to mid-2021.</p>
<ul>
<li><p>We can infer from the figure that, from 2016-2019, in general, the greatest volatility in bike usage is observed at the beginning or towards the end of the calendar year. There is likely a seasonal element to this change, less usage as the weather changes or greater usuage if there is a warm/sunny weather spell during colder months for example. It is possible to argue as well that due to lower average daily figures for these parts of the year that a fixed increase or decrease in usage would represent a larger percentage change on the figure. For example, a 2000 change in daily average bike rental for a week during the month of January will register as almost a 10% change on the weekly chart above, however an equal change for a week in July will register as almost 5% and so appear as less of change despite the raw number increase being the same as January.</p></li>
<li><p>In line with expectation, 2020 was the most volatile year in terms of weekly changes out of the measurement period. In 2020 the +/-50% level was exceeded twice,this was quite remarkable given that, for the data observed, prior to 2020 the 50% level was never exceeded. The level has been exceeded once since 2020 however, in Spring 2021 a weekly rental rate was over 50% greater than the week before - this is likely a result of the relaxation of covid restrictions for hopefully the final time and the impact this had on user behaviour.</p></li>
<li><p>Seasonal patterns aren’t clear on this figure. There is no defined period of a calendar year where usage increases or decreases for all the observed years. For example, from week 26 to 39 there is an noticeable increase in 2016 but a clear decrease of rentals for 2017 and for other years there is contradictory changes over those weeks.</p></li>
</ul>
<p>In creating your plots, we found these links useful:</p>
<ul>
<li><a href="https://ggplot2.tidyverse.org/reference/geom_ribbon.html" class="uri">https://ggplot2.tidyverse.org/reference/geom_ribbon.html</a></li>
<li><a href="https://ggplot2.tidyverse.org/reference/geom_tile.html" class="uri">https://ggplot2.tidyverse.org/reference/geom_tile.html</a></li>
<li><a href="https://ggplot2.tidyverse.org/reference/geom_rug.html" class="uri">https://ggplot2.tidyverse.org/reference/geom_rug.html</a></li>
</ul>
</div>
<div id="challenge-2-how-has-the-cpi-and-its-components-changed-over-the-last-few-years" class="section level1">
<h1>Challenge 2: How has the CPI and its components changed over the last few years?</h1>
<p>In this challenge, we will do the following:</p>
<ol style="list-style-type: decimal">
<li>We can find the <a href="https://fredaccount.stlouisfed.org/public/datalist/843">CPI components at FRED</a> and have scraped the FRED website and pulled all of the CPI components into a vector.</li>
<li>Once we have a vector of components, we can then pass it to <code>tidyquant::tq_get(get = "economic.data", from =  "2000-01-01")</code> to get all data since January 1, 2000</li>
<li>Since the data we download is an index with various starting dates, we need to calculate the yearly, or 12-month change. To do this we use the <code>lag</code> function, and specifically, <code>year_change = value/lag(value, 12) - 1</code>; this means that we are comparing the current month’s value with that 12 months ago lag(value, 12).</li>
<li>After that, we order components so the higher the yearly change, the earlier does that component appear, but make sure that <strong>All Items</strong> CPI (CPIAUCSL) appears first.</li>
<li>In order to understand the graph more easily, we also take care of some specific formatting.</li>
</ol>
<p>Having done these, we should get this graph.</p>
<p><img src="/Users/matthewtlane/Documents/LBS/CA09 Data Analytics for Finance/Lecture 1/Git Repo/my_website/images/cpi_components_since_2016.png" width="100%" style="display: block; margin: auto;" /></p>
<p>We first scrape the data from the FRED website with the following code.</p>
<pre class="r"><code>url &lt;- &quot;https://fredaccount.stlouisfed.org/public/datalist/843&quot;
library(rvest)
library(purrr)
library(htmlTable)

tables &lt;- url %&gt;% 
  read_html() %&gt;% 
  html_nodes(css=&quot;table&quot;)

CPI_components &lt;- purrr::map(tables, . %&gt;% 
             rvest::html_table(fill=TRUE)%&gt;% 
             janitor::clean_names())

cpi_components &lt;- CPI_components[[2]] %&gt;% 
  rename(component = series_id)

table &lt;- cpi_components %&gt;% 
  select(component) %&gt;% 
  pull() %&gt;% 
  tidyquant::tq_get(get = &quot;economic.data&quot;, from =  &quot;2000-01-01&quot;) %&gt;% 
  rename(component = symbol) %&gt;% 
  left_join(cpi_components, by=&quot;component&quot;) %&gt;% 
  select(title, component, date, price) %&gt;% 
  mutate(cpi_year_change = price/lag(price, 12) - 1)

glimpse(table)</code></pre>
<pre><code>## Rows: 12,753
## Columns: 5
## $ title           &lt;chr&gt; &quot;Consumer Price Index for All Urban Consumers: Airline…
## $ component       &lt;chr&gt; &quot;CUSR0000SETG01&quot;, &quot;CUSR0000SETG01&quot;, &quot;CUSR0000SETG01&quot;, …
## $ date            &lt;date&gt; 2000-01-01, 2000-02-01, 2000-03-01, 2000-04-01, 2000-…
## $ price           &lt;dbl&gt; 222, 230, 241, 240, 241, 245, 247, 250, 245, 236, 238,…
## $ cpi_year_change &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 0.0775…</code></pre>
<p>We continue with modifying the dataframe we have in order to make it ready to plot and name this new dataframe <code>table_to_plot</code>.</p>
<pre class="r"><code>table_to_plot &lt;- table %&gt;%
  filter(year(date) &gt;= 2016) %&gt;% 
  mutate(Color = if_else(cpi_year_change &gt;= 0, &quot;brown2&quot;, &quot;steelblue1&quot;)) %&gt;% #change color according to increase or decrease
  mutate(title = substr(title, 47, length(title)), #remove first characters of title that same for every component
         title = removeWords(title, &quot;in U.S. City Average&quot;), #remove words from each component
         title = fct_relevel(
           fct_reorder(title, cpi_year_change, max, .desc=TRUE), #order components according to yearly change
           &quot;All Items &quot;, after = 0)) #set &quot;All Items&quot; as first component</code></pre>
<p>Having done this, we are now able to plot the data and get a similar to the graph shown above.</p>
<pre class="r"><code>ggplot(data = table_to_plot,
       aes(x = date, y = cpi_year_change, color = Color)) +
  geom_point(size = 1, alpha = 0.6) +
  geom_smooth(se = FALSE, size = 1, color = &quot;darkgrey&quot;) + #add smooth line to show trend
  facet_wrap(~title, scales = &quot;free&quot;, nrow = 7) +
  scale_color_identity() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 0.1)) +
  labs(title = &quot;Yearly change of US CPI (All Items) and its components&quot;,
       subtitle = &quot;YoY change being &lt;span style = &#39;color: brown2;&#39;&gt;positive&lt;/span&gt; or &lt;span style = &#39;color: steelblue1;&#39;&gt;negative&lt;/span&gt;&lt;br&gt;Jan 2016 to Aug 2021&quot;, #code in HTML
       caption = &quot;Data from St. Louis FRED\nhttps://fredaccount.stlouisfed.org/public/datalist/843&quot;,
       y = &quot;YoY % Change&quot;,
       x = NULL) +
  theme_bw() +
  theme(text = element_text(size = 12),
        plot.subtitle = element_markdown(), #set to markdown so that R recognizes that code written in HTML
        plot.title = element_text(face = &quot;bold&quot;)) +
  NULL</code></pre>
<p><img src="/blogs/group_project_2_files/figure-html/unnamed-chunk-20-1.png" width="1728" style="display: block; margin: auto;" /></p>
<p>This graph contains 49 different categories, but misses some very important ones such as <em>Education</em> or <em>Medical Care</em>. Since we are not able to get data for these categories from the method above, we import the provided csv file named <code>cpi_data.csv</code> and recreate the graph, which then has more categories.</p>
<pre class="r"><code>cpi_data &lt;- read.csv(here::here(&quot;data&quot;, &quot;cpi_data.csv&quot;))

cpi_data2 &lt;- cpi_data %&gt;% 
  select(title, component, date, value) %&gt;% 
  mutate(date = ymd(date),
         cpi_year_change = value/lag(value, 12) - 1) %&gt;% 
  filter(year(date) &gt;= 2016) %&gt;%
  mutate(Color = ifelse(cpi_year_change &gt;= 0, &quot;brown2&quot;, &quot;steelblue1&quot;),
         title = substr(title, 47, length(title)), #remove first characters of title that same for every component
         title = removeWords(title, &quot;in U.S. City Average&quot;), #remove words from each component
         title = fct_relevel(
           fct_reorder(title, cpi_year_change, max, .desc=TRUE), #order components according to yearly change
           &quot;All Items &quot;, after = 0)) #set &quot;All Items&quot; as first component

ggplot(data = cpi_data2,
       aes(x = date, y = cpi_year_change, color = Color)) +
  geom_point(size = 1, alpha = 0.6) +
  geom_smooth(se = FALSE, size = 1, color = &quot;darkgrey&quot;) + #add smooth line to show trend
  facet_wrap(~title, scales = &quot;free&quot;, nrow = 7) +
  scale_color_identity() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 0.1)) +
  labs(title = &quot;Yearly change of US CPI (All Items) and its components&quot;,
       subtitle = &quot;YoY change being &lt;span style = &#39;color: brown2;&#39;&gt;positive&lt;/span&gt; or &lt;span style = &#39;color: steelblue1;&#39;&gt;negative&lt;/span&gt;&lt;br&gt;Jan 2016 to Aug 2021&quot;, #code in HTML
       caption = &quot;Data from St. Louis FRED\nhttps://fredaccount.stlouisfed.org/public/datalist/843&quot;,
       y = &quot;YoY % Change&quot;,
       x = NULL) +
  theme_bw() +
  theme(text = element_text(size = 12),
        plot.subtitle = element_markdown(), #set to markdown so that R recognizes that code written in HTML
        plot.title = element_text(face = &quot;bold&quot;)) +
  NULL</code></pre>
<p><img src="/blogs/group_project_2_files/figure-html/unnamed-chunk-21-1.png" width="1728" style="display: block; margin: auto;" /></p>
<p>This graphs is fine, but perhaps has too many sub-categories. According to <a href="https://www.bls.gov/cpi/tables/relative-importance/2020.htm">relative importance of components in the Consumer Price Indexes: U.S. city average, December 2020</a>, we can choose a smaller subset of the components and only list the major categories (Housing, Transportation, Food and beverages, Medical care, Education and communication, Recreation, and Apparel), sorted according to their relative importance.</p>
<pre class="r"><code>#create vector to store major categories sorted according to relative importance
CPI_major_categories &lt;- c(&quot;All Items &quot;, &quot;Housing &quot;,&quot;Transportation &quot;, &quot;Food and Beverages &quot;, &quot;Medical Care &quot;, &quot;Education and Communication &quot;, &quot;Recreation &quot;, &quot;Apparel &quot;)

table_major_categories &lt;- cpi_data2 %&gt;% 
  filter(title %in% CPI_major_categories) %&gt;% #filter for those categories listed above
  mutate(title = factor(title,
                        levels = CPI_major_categories,
                        labels = CPI_major_categories))

ggplot(table_major_categories,
       aes(x = date, y = cpi_year_change, color = Color)) +
  geom_point(size = 1, alpha = 0.6) +
  geom_smooth(se = FALSE, size = 1, color = &quot;darkgrey&quot;) + #add smooth line to show trend
  facet_wrap(~title, scales = &quot;free&quot;, nrow = 2) +
  scale_color_identity() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 0.1)) +
  labs(title = &quot;Yearly change of US CPI (All Items) and its major components according to their relative importance&quot;,
       subtitle = &quot;YoY change being &lt;span style = &#39;color: brown2;&#39;&gt;positive&lt;/span&gt; or &lt;span style = &#39;color: steelblue1;&#39;&gt;negative&lt;/span&gt;&lt;br&gt;Jan 2016 to Aug 2021&quot;, #code in HTML
       caption = &quot;Data from St. Louis FRED\nhttps://fredaccount.stlouisfed.org/public/datalist/843&quot;,
       y = &quot;YoY % Change&quot;,
       x = NULL) +
  theme_bw() +
  theme(text = element_text(size = 10),
        plot.subtitle = element_markdown(), #set to markdown so that R recognizes that code written in HTML
        plot.title = element_text(face = &quot;bold&quot;)) +
  NULL</code></pre>
<p><img src="/blogs/group_project_2_files/figure-html/unnamed-chunk-22-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="details" class="section level1">
<h1>Details</h1>
<ul>
<li>Who did you collaborate with: team members of Study Group A14</li>
<li>Approximately how much time did you spend on this problem set: 8 hours on average for each team member</li>
<li>What, if anything, gave you the most trouble: formatting the plots and dealing with changing data pulled from the web</li>
</ul>
<blockquote>
<p>As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?</p>
</blockquote>
<blockquote>
<p>Yes.</p>
</blockquote>
</div>
<div id="rubric" class="section level1">
<h1>Rubric</h1>
<p>Check minus (1/5): Displays minimal effort. Doesn’t complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn’t use plots appropriate for the variables being analyzed.</p>
<p>Check (3/5): Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output).</p>
<p>Check plus (5/5): Finished all components of the assignment correctly and addressed both challenges. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you’ve written additional text to describe how you interpret the output.</p>
</div>
