---
categories:  
- ""    #the front matter should be like the one found in, e.g., blog2.md. It cannot be like the normal Rmd we used
- ""
date: "2021-10-20"
description: Markdown output for 1st group project # the title that will show up once someone gets to this page
draft: false
image: pic10.jpg # save picture in \static\img\blogs. Acceptable formats= jpg, jpeg, or png . Your iPhone pics wont work

keywords: ""
slug: group_project_1 # slug is the shorthand URL address... no spaces plz
title: Group Project 1 
---



<div id="task-1-where-do-people-drink-the-most-beer-wine-and-spirits" class="section level1">
<h1>Task 1: Where Do People Drink The Most Beer, Wine And Spirits?</h1>
<p>Back in 2014, <a href="https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/">fivethiryeight.com</a> published an article on alcohol consumption in different countries. We want to use the data in this article to find out in which countries people drink the most by looking at the consumption of <strong>beer</strong>, <strong>wine</strong>, and <strong>spirits</strong>.</p>
<pre class="r"><code>library(fivethirtyeight)
data(drinks)

# or download directly
alcohol_direct &lt;- read_csv(&quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/alcohol-consumption/drinks.csv&quot;)</code></pre>
<p>Let’s take a look at our data and find whether there are any missing values and what variables we have.</p>
<pre class="r"><code>skim(alcohol_direct)</code></pre>
<table>
<caption>(#tab:glimpse_skim_data)Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">alcohol_direct</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">193</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">5</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">4</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">country</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">28</td>
<td align="right">0</td>
<td align="right">193</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">beer_servings</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">106.16</td>
<td align="right">101.14</td>
<td align="right">0</td>
<td align="right">20.0</td>
<td align="right">76.0</td>
<td align="right">188.0</td>
<td align="right">376.0</td>
<td align="left">▇▃▂▂▁</td>
</tr>
<tr class="even">
<td align="left">spirit_servings</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">80.99</td>
<td align="right">88.28</td>
<td align="right">0</td>
<td align="right">4.0</td>
<td align="right">56.0</td>
<td align="right">128.0</td>
<td align="right">438.0</td>
<td align="left">▇▃▂▁▁</td>
</tr>
<tr class="odd">
<td align="left">wine_servings</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">49.45</td>
<td align="right">79.70</td>
<td align="right">0</td>
<td align="right">1.0</td>
<td align="right">8.0</td>
<td align="right">59.0</td>
<td align="right">370.0</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">total_litres_of_pure_alcohol</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4.72</td>
<td align="right">3.77</td>
<td align="right">0</td>
<td align="right">1.3</td>
<td align="right">4.2</td>
<td align="right">7.2</td>
<td align="right">14.4</td>
<td align="left">▇▃▅▃▁</td>
</tr>
</tbody>
</table>
<p>The dataset contains one character (qualitative) variable, namely <code>country</code>, and four numeric variables (<code>beer_servings</code>, <code>spirits_servints</code>, <code>wine_servings</code> and <code>total_litres_of_pure_alcohol</code>) for which we can also see some summary statistics showing that the data is right-skewed. There are no missing values as can be seen from the <code>n_missing</code> variable that is 0 for all five variables contained in the dataset.</p>
<p>We use <code>ggplot</code> to graph the top 25 consuming countries for beer, wine and spirits respectively.</p>
<pre class="r"><code>alcohol_direct %&gt;% 
  slice_max(order_by = beer_servings,
            n = 25) %&gt;%    #pick only the top 25 countries with the most beer_servings
  ggplot(aes(x = beer_servings,
             y = fct_reorder(country, beer_servings))) + #order the country with beer_servings
  geom_col(fill = &quot;gold&quot;,
           color = &quot;grey&quot;) +
  labs(title=&quot;Top 25 beer consuming countries&quot;,
       x = &quot;Number of Beer Servings&quot;,
       y = &quot;Country&quot;) +
  coord_cartesian(xlim = c(100,NA)) +
  NULL</code></pre>
<p><img src="/blogs/group_project_1_files/figure-html/beer_plot-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>alcohol_direct %&gt;% 
  slice_max(order_by = wine_servings,
            n = 25) %&gt;%
  ggplot(aes(x = wine_servings,
             y = fct_reorder(country, wine_servings))) +
  geom_col(fill = &quot;deeppink4&quot;,    #choose the most wine-like color in the list of color table
           color = &quot;grey&quot;) +
  labs(title = &quot;Top 25 wine consuming countries&quot;,
       x = &quot;Number of Wine Servings&quot;,
       y = &quot;Country&quot;) +
  coord_cartesian(xlim = c(100,NA)) +
  NULL</code></pre>
<p><img src="/blogs/group_project_1_files/figure-html/wine_plot-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>alcohol_direct %&gt;% 
  slice_max(order_by = spirit_servings,
            n = 25) %&gt;%
  ggplot(aes(x = spirit_servings,
             y = fct_reorder(country,
                           spirit_servings))) +
  geom_col(fill = &quot;cornsilk3&quot;, color = &quot;grey&quot;) +
  labs(title = &quot;Top 25 spirits consuming countries&quot;,
       x = &quot;Number of Spirit Servings&quot;,
       y = &quot;Country&quot;) +
  coord_cartesian(xlim = c(100,NA)) +
  NULL</code></pre>
<p><img src="/blogs/group_project_1_files/figure-html/spirit_plot-1.png" width="672" style="display: block; margin: auto;" /></p>
<blockquote>
<p>We can infer from the above graphs…</p>
</blockquote>
<p>On a high-level overview we observe different countries being in the top spots for each type of alcohol.</p>
<ul>
<li><p>Beginning with <strong>beer</strong> consumption, the top 5 spots are dominated by central and eastern European countries with a few “surprises”. Namibia apparently has a long history of beer production. According to CNN, the first Namibian brewery opened in 1900, with every ethnic group in Africa having its own methods to create the famous beverage. Nowadays local beer is a tourist attraction and a large source of revenues for both Namibia and Ghana, explaining their placement in the list. As for the rest of the countries comprising the top 5, it is known that beer is very inexpensive there, with Czech Republic offering beer at a lower price than water in some pubs and restaurants.</p></li>
<li><p>Moving to <strong>wine</strong>, the top spot is with France (arguably) highly anticipated. Bordeaux, a rural city in France has got some of the world’s finest wineries, with its wine being beloved both locally and internationally. Similar reasoning applies to Portugal. Douro Port is the country’s most famous wine, with many casual drinkers switching to premium wines, according to wininteligence.com. What is definitely worthy of commentary is Andorra’s placement on the list, because of its small population ranking it amongst the six European countries with the least residents. Again, as Andorra is a famous tourist destination because of its ski resorts, the high wine consumption is mostly attributed to tourists.</p></li>
<li><p>Finally, regarding <strong>spirits</strong> we have mixed signals about the top 5. Although we would most definitely expect eastern countries such as Russia, the motherland of Vodka to be high on the list, Grenada seems to be number one, with another Caribbean country, St. Lucia, following on fifth. According to jamaicaobserver.com, both countries seem to have problems with illegal consumption since 2014, when the WHO declared Grenada as the country with the highest alcohol per capita consumption in the Caribbean.</p></li>
</ul>
</div>
<div id="task-2-analysis-of-movies--imdb-dataset" class="section level1">
<h1>Task 2: Analysis of movies- IMDB dataset</h1>
<p>We will look at a subset sample of movies, taken from the <a href="https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset">Kaggle IMDB 5000 movie dataset</a>.</p>
<pre class="r"><code>movies &lt;- read.csv(here::here(&quot;data&quot;, &quot;movies.csv&quot;))
glimpse(movies)</code></pre>
<pre><code>## Rows: 2,961
## Columns: 11
## $ title               &lt;chr&gt; &quot;Avatar&quot;, &quot;Titanic&quot;, &quot;Jurassic World&quot;, &quot;The Avenge…
## $ genre               &lt;chr&gt; &quot;Action&quot;, &quot;Drama&quot;, &quot;Action&quot;, &quot;Action&quot;, &quot;Action&quot;, &quot;…
## $ director            &lt;chr&gt; &quot;James Cameron&quot;, &quot;James Cameron&quot;, &quot;Colin Trevorrow…
## $ year                &lt;int&gt; 2009, 1997, 2015, 2012, 2008, 1999, 1977, 2015, 20…
## $ duration            &lt;int&gt; 178, 194, 124, 173, 152, 136, 125, 141, 164, 93, 1…
## $ gross               &lt;int&gt; 760505847, 658672302, 652177271, 623279547, 533316…
## $ budget              &lt;int&gt; 237000000, 200000000, 150000000, 220000000, 185000…
## $ cast_facebook_likes &lt;int&gt; 4834, 45223, 8458, 87697, 57802, 37723, 13485, 920…
## $ votes               &lt;int&gt; 886204, 793059, 418214, 995415, 1676169, 534658, 9…
## $ reviews             &lt;int&gt; 3777, 2843, 1934, 2425, 5312, 3917, 1752, 1752, 35…
## $ rating              &lt;dbl&gt; 7.9, 7.7, 7.0, 8.1, 9.0, 6.5, 8.7, 7.5, 8.5, 7.2, …</code></pre>
<p>Let’s see if there are any missing values in the dataset.</p>
<pre class="r"><code>skim(movies) #no missing values (n_missing is 0 for all variables)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-1">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">movies</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">2961</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">11</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">3</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">8</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">title</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">83</td>
<td align="right">0</td>
<td align="right">2907</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">genre</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">5</td>
<td align="right">11</td>
<td align="right">0</td>
<td align="right">17</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">director</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">32</td>
<td align="right">0</td>
<td align="right">1366</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table style="width:100%;">
<colgroup>
<col width="18%" />
<col width="9%" />
<col width="12%" />
<col width="8%" />
<col width="8%" />
<col width="6%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="5%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">year</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">2.00e+03</td>
<td align="right">9.95e+00</td>
<td align="right">1920.0</td>
<td align="right">2.00e+03</td>
<td align="right">2.00e+03</td>
<td align="right">2.01e+03</td>
<td align="right">2.02e+03</td>
<td align="left">▁▁▁▂▇</td>
</tr>
<tr class="even">
<td align="left">duration</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1.10e+02</td>
<td align="right">2.22e+01</td>
<td align="right">37.0</td>
<td align="right">9.50e+01</td>
<td align="right">1.06e+02</td>
<td align="right">1.19e+02</td>
<td align="right">3.30e+02</td>
<td align="left">▃▇▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">gross</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">5.81e+07</td>
<td align="right">7.25e+07</td>
<td align="right">703.0</td>
<td align="right">1.23e+07</td>
<td align="right">3.47e+07</td>
<td align="right">7.56e+07</td>
<td align="right">7.61e+08</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">budget</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4.06e+07</td>
<td align="right">4.37e+07</td>
<td align="right">218.0</td>
<td align="right">1.10e+07</td>
<td align="right">2.60e+07</td>
<td align="right">5.50e+07</td>
<td align="right">3.00e+08</td>
<td align="left">▇▂▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">cast_facebook_likes</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1.24e+04</td>
<td align="right">2.05e+04</td>
<td align="right">0.0</td>
<td align="right">2.24e+03</td>
<td align="right">4.60e+03</td>
<td align="right">1.69e+04</td>
<td align="right">6.57e+05</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">votes</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1.09e+05</td>
<td align="right">1.58e+05</td>
<td align="right">5.0</td>
<td align="right">1.99e+04</td>
<td align="right">5.57e+04</td>
<td align="right">1.33e+05</td>
<td align="right">1.69e+06</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">reviews</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">5.03e+02</td>
<td align="right">4.94e+02</td>
<td align="right">2.0</td>
<td align="right">1.99e+02</td>
<td align="right">3.64e+02</td>
<td align="right">6.31e+02</td>
<td align="right">5.31e+03</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">rating</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">6.39e+00</td>
<td align="right">1.05e+00</td>
<td align="right">1.6</td>
<td align="right">5.80e+00</td>
<td align="right">6.50e+00</td>
<td align="right">7.10e+00</td>
<td align="right">9.30e+00</td>
<td align="left">▁▁▆▇▁</td>
</tr>
</tbody>
</table>
<p>Looking at the output above and the <code>n_missing</code> variable, we can see that this is 0 for all variables, meaning that there are no missing values.</p>
<p>Then we want to find out if there are duplicate entries in the dataset.</p>
<pre class="r"><code>#filter for duplicates
movies %&gt;% 
  filter(duplicated(title)) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 54
## Columns: 11
## $ title               &lt;chr&gt; &quot;Spider-Man 3&quot;, &quot;Alice in Wonderland&quot;, &quot;Oz the Gre…
## $ genre               &lt;chr&gt; &quot;Action&quot;, &quot;Adventure&quot;, &quot;Adventure&quot;, &quot;Drama&quot;, &quot;Dram…
## $ director            &lt;chr&gt; &quot;Sam Raimi&quot;, &quot;Tim Burton&quot;, &quot;Sam Raimi&quot;, &quot;Kenneth B…
## $ year                &lt;int&gt; 2007, 2010, 2013, 2015, 2008, 2014, 2001, 2015, 20…
## $ duration            &lt;int&gt; 156, 108, 130, 105, 122, 101, 119, 94, 94, 125, 10…
## $ gross               &lt;int&gt; 336530303, 334185206, 234903076, 201148159, 191449…
## $ budget              &lt;int&gt; 258000000, 200000000, 215000000, 95000000, 3700000…
## $ cast_facebook_likes &lt;int&gt; 46055, 79957, 73441, 4671, 44060, 2690, 2776, 1788…
## $ votes               &lt;int&gt; 383071, 306336, 175413, 103749, 348007, 167089, 17…
## $ reviews             &lt;int&gt; 2294, 1187, 1036, 666, 1885, 839, 1598, 379, 379, …
## $ rating              &lt;dbl&gt; 6.2, 6.5, 6.4, 7.0, 5.2, 5.9, 5.7, 6.7, 6.7, 6.8, …</code></pre>
<p>The output shows that we have 54 duplicates based on the title of the movies. After having checked the duplicates (see example below), we have come to the conclusion that, while the duplicated entries are not 100% identical, they only differ in terms of the number of votes and reviews and thereby to a very little extent. This is why we remove the 54 duplicates identified above as a next step from our dataset.</p>
<pre class="r"><code>#check in which way duplicate observations differ with example of Cinderalla
movies %&gt;% 
  filter(title==&quot;Cinderella&quot;)</code></pre>
<pre><code>##        title genre        director year duration     gross   budget
## 1 Cinderella Drama Kenneth Branagh 2015      105 201148159 95000000
## 2 Cinderella Drama Kenneth Branagh 2015      105 201148159 95000000
##   cast_facebook_likes  votes reviews rating
## 1                4671 103737     665      7
## 2                4671 103749     666      7</code></pre>
<p>Let’s count movies by genre and see which genre has the most entries in our dataset. Our guess would be comedy because it’s the easiest type for entertainment.</p>
<pre class="r"><code>movies &lt;- movies %&gt;% 
  distinct(title,
           .keep_all = TRUE) #Removing the 54 duplicates

movies %&gt;% 
  count(genre,
        sort = TRUE) #Count by genre and sort in descending order</code></pre>
<pre><code>##          genre   n
## 1       Comedy 844
## 2       Action 719
## 3        Drama 484
## 4    Adventure 281
## 5        Crime 198
## 6    Biography 135
## 7       Horror 128
## 8    Animation  35
## 9      Fantasy  26
## 10 Documentary  25
## 11     Mystery  15
## 12      Sci-Fi   7
## 13      Family   3
## 14     Musical   2
## 15     Romance   2
## 16     Western   2
## 17    Thriller   1</code></pre>
<p>We now produce a table with the average gross earning and budget (<code>gross</code> and <code>budget</code>) by genre and try to calculate the return on budget for each genre.</p>
<p>Our approach is first creating a variable named <code>return_on_budget</code> for every movie and then calculating its average in each genre. In this way we can better represent how profitable a movie is on average in each genre. An alternative approach would be using average earning over average budget, but the result would skew to those movies with bigger budget, so this would be somewhat misleading.</p>
<pre class="r"><code>#Producing a table of each genre&#39;s average gross, budget and return
movies %&gt;% 
  mutate(return_on_budget = gross/budget) %&gt;%   
  group_by(genre) %&gt;% 
    summarise(gross_avg = mean(gross),
              budget_avg = mean(budget),
              return_on_budget_avg = mean(return_on_budget)) %&gt;% 
    arrange(desc(return_on_budget_avg))</code></pre>
<pre><code>## # A tibble: 17 × 4
##    genre        gross_avg budget_avg return_on_budget_avg
##    &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;                &lt;dbl&gt;
##  1 Horror       37782310.  13804379.             86.1    
##  2 Biography    45201805.  28543696.             22.3    
##  3 Musical      92084000    3189500              18.8    
##  4 Family      149160478.  14833333.             14.1    
##  5 Documentary  17353973.   5887852.              8.70   
##  6 Western      20821884    3465000               7.06   
##  7 Fantasy      41902674.  18484615.              6.10   
##  8 Animation    98433792.  61701429.              5.01   
##  9 Comedy       42487808.  24458506.              3.70   
## 10 Romance      31264848.  25107500               3.17   
## 11 Drama        36754959.  25832605.              2.98   
## 12 Mystery      69117136.  41500000               2.90   
## 13 Adventure    94350236.  64692313.              2.44   
## 14 Crime        37601525.  26527405.              2.19   
## 15 Action       86270343.  70774558.              1.93   
## 16 Sci-Fi       29788371.  27607143.              1.58   
## 17 Thriller         2468     300000               0.00823</code></pre>
<p>Horror movies, biographies and musical movies are the top 3 in terms of profitability.</p>
<p>Let us also find out who are the directors that create the highest revenue-generating movies.</p>
<pre class="r"><code>#Produce a table of directors regarding their gross
movies %&gt;% 
  group_by(director) %&gt;% 
  summarise(gross_total = sum(gross),
            gross_mean = mean(gross),
            gross_median = median(gross),
            gross_sd = sd(gross)) %&gt;% 
  slice_max(order_by = gross_total,
            n = 15)</code></pre>
<pre><code>## # A tibble: 15 × 5
##    director          gross_total gross_mean gross_median   gross_sd
##    &lt;chr&gt;                   &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;
##  1 Steven Spielberg   4014061704 174524422.   164435221  101421051.
##  2 Michael Bay        2195443511 182953626.   168468240. 125789167.
##  3 James Cameron      1909725910 318287652.   175562880. 309171337.
##  4 Christopher Nolan  1813227576 226653447    196667606. 187224133.
##  5 George Lucas       1741418480 348283696    380262555  146193880.
##  6 Robert Zemeckis    1619309108 124562239.   100853835   91300279.
##  7 Tim Burton         1557078534 111219895.    69791834   99304293.
##  8 Sam Raimi          1443167519 180395940.   138480208  174705230.
##  9 Clint Eastwood     1378321100  72543216.    46700000   75487408.
## 10 Francis Lawrence   1358501971 271700394.   281666058  135437020.
## 11 Ron Howard         1335988092 111332341    101587923   81933761.
## 12 Gore Verbinski     1329600995 189942999.   123207194  154473822.
## 13 Andrew Adamson     1137446920 284361730    279680930. 120895765.
## 14 Shawn Levy         1129750988 102704635.    85463309   65484773.
## 15 Ridley Scott       1128857598  80632686.    47775715   68812285.</code></pre>
<p>Apparently Steven Spielberg leads the ranking, however, he is not one of the directors with the highest mean revenue (<code>gross_mean</code>) or median revenue (<code>gross_median</code>) or lowest standard deviation of revenues (<code>gross_sd</code>).</p>
<p>How about ratings? Let’s see how each genre is regarded by its viewers.</p>
<pre class="r"><code>#Produce a table showing each genre&#39;s rating information
movies %&gt;% 
  group_by(genre) %&gt;% 
  summarise(rating_mean = mean(rating),
            rating_min = min(rating),
            rating_max = max(rating),
            rating_median = median(rating),
            rating_sd = sd(rating)) %&gt;% 
  arrange(desc(rating_mean))</code></pre>
<pre><code>## # A tibble: 17 × 6
##    genre       rating_mean rating_min rating_max rating_median rating_sd
##    &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;
##  1 Biography          7.11        4.5        8.9          7.2      0.760
##  2 Crime              6.92        4.8        9.3          6.9      0.853
##  3 Mystery            6.84        4.6        8.5          6.7      0.910
##  4 Musical            6.75        6.3        7.2          6.75     0.636
##  5 Drama              6.74        2.1        8.8          6.8      0.915
##  6 Documentary        6.66        1.6        8.5          7.4      1.77 
##  7 Sci-Fi             6.66        5          8.2          6.4      1.09 
##  8 Animation          6.65        4.5        8            6.9      0.968
##  9 Romance            6.65        6.2        7.1          6.65     0.636
## 10 Adventure          6.51        2.3        8.6          6.6      1.11 
## 11 Family             6.5         5.7        7.9          5.9      1.22 
## 12 Action             6.23        2.1        9            6.3      1.04 
## 13 Comedy             6.11        1.9        8.8          6.2      1.02 
## 14 Fantasy            6.08        4.3        7.9          6.2      0.953
## 15 Horror             5.79        3.6        8.5          5.85     0.987
## 16 Western            5.7         4.1        7.3          5.7      2.26 
## 17 Thriller           4.8         4.8        4.8          4.8     NA</code></pre>
<pre class="r"><code># Graph the distribution of ratings for all movies
ggplot(movies, aes(x=rating)) +
  geom_histogram(bins = 20,
                 color = &quot;grey&quot;,
                 fill = &quot;lightblue&quot;) +
  labs(title = &quot;Distribution of ratings for all movies&quot;,
       y = &quot;Number of Movies&quot;,
       x = &quot;Rating Score&quot;) +
  NULL</code></pre>
<p><img src="/blogs/group_project_1_files/figure-html/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Graph the distribution of ratings per genre (Thriller only includes 1 movie, so no graph shown)
ggplot(movies,
       aes(x = rating)) +
#Density plot to make comparison among genres easier, considering difference in sample sizes
  geom_density(color = &quot;grey&quot;,    
               fill = &quot;lightblue&quot;) +
  facet_wrap(~ genre) +
  labs(title = &quot;Distribution of ratings per genre&quot;,
       y = NULL,
       x = &quot;Rating Score&quot;) +
  NULL</code></pre>
<p><img src="/blogs/group_project_1_files/figure-html/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#check why no curve for genre Thriller
movies %&gt;% 
  filter(genre==&quot;Thriller&quot;)</code></pre>
<pre><code>##       title    genre     director year duration gross budget
## 1 Locker 13 Thriller Bruce Dellis 2014       95  2468 300000
##   cast_facebook_likes votes reviews rating
## 1                2048   241      15    4.8</code></pre>
<p>We only have 1 movie categorized as <strong>thriller</strong> in the dataset, so there’s no density plot for thriller. We observe that genres like <strong>family</strong>, <strong>western</strong>, <strong>romance</strong> and <strong>musical</strong> have multimodal distribution, because they only have single-digit samples. If we do not take these multimodal genres into account, the best-regarded genres are <strong>biography</strong>, <strong>crime</strong> and <strong>documentary</strong>, because their rating scores are the most concentrated around the peak in distribution and their peaks are highest-scored. The box plot also indicates the same, as depicted below.</p>
<pre class="r"><code>#Box plot to make the comparison again, telling the same conclusion (biography wins) in a more quantitative perspective

ggplot(movies,
       aes(x = rating)) +
  geom_boxplot(color = &quot;grey&quot;,    
               fill = &quot;lightblue&quot;) +
  facet_wrap(~ genre) +
  labs(title = &quot;Distribution of ratings per genre&quot;,
       y = NULL,
       x = &quot;Rating Score&quot;) +
  NULL</code></pre>
<p><img src="/blogs/group_project_1_files/figure-html/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The importance of social media nowadays could mean that the number of facebook likes that the cast of a movie has is likely to be a good predictor of how much money a movie will make at the box office. We therefore examine the relationship between <code>gross</code> and <code>cast_facebook_likes</code> and find out if this is true.</p>
<pre class="r"><code>#Create a scatterplot and a trend line
ggplot(movies,
       aes(x = cast_facebook_likes,
           y = gross))+
  geom_point(alpha = 0.5) + 
  geom_smooth() +
  
  #Log scale presents a more significant correlation
  #Don&#39;t want to use scientific notation here
  scale_y_log10(labels = scales::comma) + 
  scale_x_log10(labels = scales::comma) +
  
  labs (x = &quot;Number of Facebook Likes for Cast&quot;,
        y = &quot;Gross Revenue of Movie&quot;,
        title = &quot;Relationship between facebook likes of cast and gross of movie&quot;) +
  NULL</code></pre>
<p><img src="/blogs/group_project_1_files/figure-html/gross_on_fblikes2-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Calculate correlation coefficient to understand strength of relationship
movies %&gt;% 
  select(cast_facebook_likes,gross) %&gt;% 
  cor()</code></pre>
<pre><code>##                     cast_facebook_likes gross
## cast_facebook_likes               1.000 0.208
## gross                             0.208 1.000</code></pre>
<p>Looking at the scatterplot that uses a logarithmic scale as well as the correlation coefficient, there is a certain positive relationship between the number of Facebook likes the cast of a movie has and the revenue this movie will make at the box office. This can be taken as a sign that a more popular or social-media active cast will create higher revenues. However, this relationship is with a correlation coefficient of 0.213 not very strong.</p>
<p>We also examine the relationship between <code>gross</code> and <code>budget</code> to see whether the budget is likely to be a better predictor of how much money a movie will make at the box office.</p>
<pre class="r"><code>#Create a scatterplot with a trend line
ggplot(movies,
       aes(x = budget,
           y = gross))+
  geom_point() + 
  scale_y_log10(labels = scales::comma) +
  scale_x_log10(labels = scales::comma) +
  geom_smooth() +
  labs (title = &quot;Relationship between budget and revenue of movie&quot;,
        x = &quot;Budget of Movie&quot;,
        y = &quot;Gross Revenue of Movie&quot;) +
  NULL</code></pre>
<p><img src="/blogs/group_project_1_files/figure-html/gross_on_budget-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Calculate the correlation coefficient to understand strength of relationship
movies %&gt;% 
  select(budget,gross) %&gt;% 
  cor()</code></pre>
<pre><code>##        budget gross
## budget  1.000 0.639
## gross   0.639 1.000</code></pre>
<p>There is a clear positive relationship between the budget a movie has and the revenues it achieves at the box office (correlation coefficient of 0.641). This means that movies with a higher budget will also be the ones that make more revenues. We can infer that budget is a better predictor of gross revenues than the number of Facebook likes of the cast.</p>
<p>The third possible predictor of gross revenues of a movie could be its IMDB rating. Let’s examine this relationship with the same method and facet it by genre.</p>
<pre class="r"><code>#Create a scatterplot with a trend line per genre
ggplot(movies,
       aes(x = rating,
           y = gross))+
  geom_point() + 
  geom_smooth() +
  facet_wrap(~genre) +
  scale_y_continuous(labels = scales::comma) +
  labs (title = &quot;Relationship between the rating of a movie and its gross revenue&quot;,
        x = &quot;Rating of Movie&quot;,
        y = &quot;Gross Revenue of Movie&quot;) +
  NULL</code></pre>
<p><img src="/blogs/group_project_1_files/figure-html/gross_on_rating-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Calculate correlation coefficient to understand strength of relationship (for all genres together)
movies %&gt;% 
  select(gross,rating) %&gt;% 
  cor()</code></pre>
<pre><code>##        gross rating
## gross  1.000  0.274
## rating 0.274  1.000</code></pre>
<p>Looking at the scatterplots above, a certain positive relationship between the ratings of a movie and its gross revenue can be inferred, especially for the genres <strong>action</strong>, <strong>adventure</strong>, <strong>comedy</strong>, <strong>crime</strong>, <strong>drama</strong> and <strong>horror</strong>. As the steepness of the smooth line increases the higher the ratings get for these genres, this means that an increase in rating for movies that are already highly ranked will bring a higher increase in revenues than a rating increase for movies that are rated low. However, for some genres such as <strong>animation</strong> or <strong>documentary</strong> there is no clear relationship, which hints at the fact that people tend to use ratings for some genres more than others to decide whether to watch the movie.</p>
</div>
<div id="task-3-returns-of-financial-stocks" class="section level1">
<h1>Task 3: Returns of financial stocks</h1>
<p>We will use the <code>tidyquant</code> package to download historical data of stock prices, calculate returns, and examine the distribution of returns.</p>
<p>We must first identify which stocks we want to download data for, and for this we must know their ticker symbol. The file <code>nyse.csv</code> contains 508 stocks listed on the NYSE, their ticker symbol, name, the IPO year, and the sector and industry the company is in.</p>
<pre class="r"><code>nyse &lt;- read_csv(here::here(&quot;data&quot;,&quot;nyse.csv&quot;))</code></pre>
<p>We create a table and a bar plot that shows the number of companies per sector, in descending order.</p>
<pre class="r"><code>company_sector_sum &lt;- nyse %&gt;% #create dataframe that counts # of companies per sector
    group_by(sector) %&gt;%
    count(sort = TRUE) 
company_sector_sum   #show the table</code></pre>
<pre><code>## # A tibble: 12 × 2
## # Groups:   sector [12]
##    sector                    n
##    &lt;chr&gt;                 &lt;int&gt;
##  1 Finance                  97
##  2 Consumer Services        79
##  3 Public Utilities         60
##  4 Capital Goods            45
##  5 Health Care              45
##  6 Energy                   42
##  7 Technology               40
##  8 Basic Industries         39
##  9 Consumer Non-Durables    31
## 10 Miscellaneous            12
## 11 Transportation           10
## 12 Consumer Durables         8</code></pre>
<pre class="r"><code>ggplot(company_sector_sum,
       aes(x = reorder(sector, -n),
           y = n)) + #plot output from above
  theme(axis.text.x = element_text(angle = 45,  # Tilt the x-label at an angle of 45
                                   hjust = 1)) +
  geom_bar(fill = &quot;lightblue&quot;,
           color = &quot;grey&quot;,
           stat = &quot;identity&quot;) +
  labs(title = &quot;Number of companies per sector&quot;,
       x = &quot;Sector&quot;,
       y = &quot;Number of Companies&quot;) +
  NULL</code></pre>
<p><img src="/blogs/group_project_1_files/figure-html/companies_per_sector-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Next, let’s choose the <a href="https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average">Dow Jones Industrial Aveareg (DJIA)</a> stocks and their ticker symbols and download some data. Besides the thirty stocks that make up the DJIA, we will also add <code>SPY</code> which is an SP500 ETF.</p>
<pre class="r"><code>djia_url &lt;- &quot;https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average&quot;

#get tables that exist on URL
tables &lt;- djia_url %&gt;% 
  read_html() %&gt;% 
  html_nodes(css=&quot;table&quot;)

# parse HTML tables into a dataframe called djia. 
# Use purr::map() to create a list of all tables in URL
djia &lt;- map(tables, . %&gt;% 
               html_table(fill=TRUE)%&gt;% 
               clean_names())

# constituents
table1 &lt;- djia[[2]] %&gt;% # the second table on the page contains the ticker symbols
  mutate(date_added = ymd(date_added),
         
         # if a stock is listed on NYSE, its symbol is, e.g., NYSE: MMM
         # We will get prices from yahoo finance which requires just the ticker
         
         # if symbol contains &quot;NYSE*&quot;, the * being a wildcard
         # then we jsut drop the first 6 characters in that string
         ticker = ifelse(str_detect(symbol, &quot;NYSE*&quot;),
                          str_sub(symbol,7,11),
                          symbol)
         )

# we need a vector of strings with just the 30 tickers + SPY
tickers &lt;- table1 %&gt;% 
  select(ticker) %&gt;% 
  pull() %&gt;% # pull() gets them as a sting of characters
  c(&quot;SPY&quot;) # and lets us add SPY, the SP500 ETF</code></pre>
<p>Now let us downlaod prices for all 30 DJIA consituents and the <code>SPY</code> ETF that tracks SP500 since January 1, 2020</p>
<pre class="r"><code># Notice the cache=TRUE argument in the chunk options. Because getting data is time consuming, # cache=TRUE means that once it downloads data, the chunk will not run again next time you knit your Rmd

myStocks &lt;- tickers %&gt;% 
  tq_get(get  = &quot;stock.prices&quot;,
         from = &quot;2000-01-01&quot;,
         to   = Sys.Date()) %&gt;% # Sys.Date() returns today&#39;s price
  group_by(symbol) 

glimpse(myStocks) # examine the structure of the resulting data frame</code></pre>
<pre><code>## Rows: 162,049
## Columns: 8
## Groups: symbol [31]
## $ symbol   &lt;chr&gt; &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;, &quot;MMM&quot;…
## $ date     &lt;date&gt; 2000-01-03, 2000-01-04, 2000-01-05, 2000-01-06, 2000-01-07, …
## $ open     &lt;dbl&gt; 48.0, 46.4, 45.6, 47.2, 50.6, 50.2, 50.4, 51.0, 50.7, 50.4, 4…
## $ high     &lt;dbl&gt; 48.2, 47.4, 48.1, 51.2, 51.9, 51.8, 51.2, 51.8, 50.9, 50.5, 4…
## $ low      &lt;dbl&gt; 47.0, 45.3, 45.6, 47.2, 50.0, 50.0, 50.2, 50.4, 50.2, 49.5, 4…
## $ close    &lt;dbl&gt; 47.2, 45.3, 46.6, 50.4, 51.4, 51.1, 50.2, 50.4, 50.4, 49.7, 4…
## $ volume   &lt;dbl&gt; 2173400, 2713800, 3699400, 5975800, 4101200, 3863800, 2357600…
## $ adjusted &lt;dbl&gt; 27.2, 26.1, 26.9, 29.0, 29.6, 29.4, 28.9, 29.0, 29.0, 28.6, 2…</code></pre>
<p>Given the adjusted closing prices, our first step is to calculate daily and monthly returns.</p>
<pre class="r"><code>#calculate daily returns
myStocks_returns_daily &lt;- myStocks %&gt;%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = &quot;daily&quot;, 
               type       = &quot;log&quot;,
               col_rename = &quot;daily_returns&quot;,
               cols = c(nested.col))  

#calculate monthly returns
myStocks_returns_monthly &lt;- myStocks %&gt;%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = &quot;monthly&quot;, 
               type       = &quot;arithmetic&quot;,
               col_rename = &quot;monthly_returns&quot;,
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual &lt;- myStocks %&gt;%
  group_by(symbol) %&gt;%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = &quot;yearly&quot;, 
               type       = &quot;arithmetic&quot;,
               col_rename = &quot;yearly_returns&quot;,
               cols = c(nested.col))</code></pre>
<p>We then summarise monthly returns for each of the stocks and <code>SPY</code>, including its min, max, median, mean and standard deviation.</p>
<pre class="r"><code>#Create the summary dataset
myStocks_monthly_returns &lt;- myStocks_returns_monthly %&gt;%
  group_by(symbol) %&gt;% 
  summarise(Mean = mean(monthly_returns),
            Minimum = min(monthly_returns),
            Maximum = max(monthly_returns),
            Median = median(monthly_returns),
            StandardDeviation = sd(monthly_returns)) %&gt;%
  arrange(desc(Mean))

#Display the summary in table
myStocks_monthly_returns</code></pre>
<pre><code>## # A tibble: 31 × 6
##    symbol   Mean Minimum Maximum Median StandardDeviation
##    &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;
##  1 AAPL   0.0269  -0.577   0.454 0.0352            0.115 
##  2 CRM    0.0262  -0.360   0.403 0.0205            0.110 
##  3 V      0.0199  -0.196   0.338 0.0256            0.0660
##  4 UNH    0.0193  -0.306   0.266 0.0232            0.0700
##  5 NKE    0.0164  -0.375   0.396 0.0169            0.0758
##  6 DOW    0.0160  -0.276   0.255 0.0386            0.109 
##  7 CAT    0.0144  -0.353   0.350 0.0141            0.0899
##  8 BA     0.0126  -0.458   0.459 0.0165            0.0925
##  9 MSFT   0.0115  -0.344   0.408 0.0177            0.0820
## 10 JPM    0.0111  -0.281   0.255 0.0161            0.0868
## # … with 21 more rows</code></pre>
<p>We then create a density plot for each of the stocks to see their performances.</p>
<pre class="r"><code>#Create a density plot of monthly returns
ggplot(myStocks_returns_monthly,
       aes(x = monthly_returns)) + 
  geom_density() +
  facet_wrap(~symbol) +
  scale_x_continuous(labels = scales::percent_format(accuracy=1))+  #x-axis in percentage
  labs(title=&quot;Distribution of monthly returns of selected stocks&quot;,
        x = &quot;Monthly Return&quot;,
        y=&quot;Density&quot;)+
  NULL</code></pre>
<p><img src="/blogs/group_project_1_files/figure-html/density_monthly_returns-1.png" width="672" style="display: block; margin: auto;" /></p>
<blockquote>
<p>We can infer from the above table and plot…</p>
</blockquote>
<ul>
<li><p>The plot shows that most return distributions are non-normal, evidencing a high level of kurtosis. It also shows that all of the companies have positive average returns, albeit some are more marginally positive than others. AXP has the highest maximum monthly return and AAPL has the lowest minimum.</p></li>
<li><p>The riskiest stock, surprisingly, is <strong>Apple</strong>. This is because it has the highest standard deviation (and lowest kurtosis) and lowest minimum return out of index and S&amp;P500 tracker. This is somewhat counterintuitive because Apple also has the highest mean return out of the stock selection, but as risk in finance is measured by volatility we conclude that Apple is the riskiest.</p></li>
<li><p>We believe that, in line with financial theory, that the <strong>SPY</strong> is the least risky asset among those examined. This is because its distribution is heavily concentrated around the 0% return demonstrated by the highest density out of the group at a single point. It also has the lowest standard deviation, again expected, due to the diversified nature of the index product. Thus, we conclude that the SPY is the least risky out of the selection.</p></li>
</ul>
<p>Finally, we want to make a scatter plot that shows the expected monthly return and risk of a stock.</p>
<pre class="r"><code>#Create a scatterplot with mean and sd of monthly returns
ggplot(myStocks_monthly_returns,
       aes(x = StandardDeviation,
           y = Mean)) +
  geom_point()+
  geom_smooth(method = &quot;lm&quot;,
              se=FALSE)+
  ggrepel::geom_text_repel(aes(label = symbol)) +
  scale_x_continuous(labels = scales::percent_format(accuracy = 0.1))+
  scale_y_continuous(labels = scales::percent_format(accuracy = 0.1))+
  labs(title=&quot;Expected monthly return of stocks and their risk&quot;,
        x = &quot;Standard Deviation&quot;,
        y=&quot;Expected Monthly Return&quot;)+
  NULL</code></pre>
<p><img src="/blogs/group_project_1_files/figure-html/risk_return_plot-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>What can you infer from this plot? Are there any stocks which, while being riskier, do not have a higher expected return?</p>
<blockquote>
<p>We can infer from the above plot…</p>
</blockquote>
<ul>
<li><p>The plot shows that, in general, the higher the mean return the higher the risk - or standard deviation. This is as expected because financial theory implies that for higher returns investors must anticipate greater volatility/risk.</p></li>
<li><p>There are several stocks that sit below the line of best fit and therefore offer investors returns that do not adequately compensate for the greater risk that is undertaken. These stocks are <strong>DOW</strong>, <strong>CSCO</strong>, <strong>INTC</strong>. The worst stock for this is <strong>CSCO</strong> which offers a very low mean return but offers the fourth highest level of volatility out of the group - this would be a very bad investment. There are a few stocks that outperform the expected mean-volatility trade off, such as <strong>V</strong> and <strong>UNH</strong>, these stocks offer greater returns but with below expected volatility.</p></li>
</ul>
</div>
<div id="task-4-is-inflation-transitory" class="section level1">
<h1>Task 4: Is inflation transitory?</h1>
<p>A recent study by the Bank for International Settlements (BIS) claimed that the <a href="https://www.bloomberg.com/news/articles/2021-09-20/current-inflation-spike-is-just-transitory-new-bis-study-argues">Current Inflation Spike Is Just Transitory</a>. As the article says,</p>
<blockquote>
<p>The surge in inflation seen across major economies is probably short lived because it’s confined to just a few sectors of the economy, according to the Bank for International Settlements.</p>
</blockquote>
<blockquote>
<p>New research by the BIS’s Claudio Borio, Piti Disyatat, Egon Zakrajsek and Dora Xia adds to one of the hottest debates in economics – how long the current surge in consumer prices will last. Both Federal Reserve Chair Jerome Powell and his euro-area counterpart Christine Lagarde have said the pickup is probably transitory, despite a snarled global supply chain and a spike in energy prices.</p>
</blockquote>
<p>To better understand inflation, we want to use CPI and 10-year yield to produce the following graph:</p>
<p><img src="/Users/matthewtlane/Documents/LBS/CA09%20Data%20Analytics%20for%20Finance/Lecture%201/Git%20Repo/ca09.mfa2022/images/cpi_10year.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Our two variables are:</p>
<ul>
<li><a href="https://fred.stlouisfed.org/series/CPIAUCSL">Consumer Price Index for All Urban Consumers: All Items in U.S. City Average</a></li>
<li><a href="https://fred.stlouisfed.org/series/GS10">10-Year Treasury Constant Maturity Rate</a></li>
</ul>
<pre class="r"><code>cpi  &lt;-   tq_get(&quot;CPIAUCSL&quot;, get = &quot;economic.data&quot;,
                       from = &quot;1980-01-01&quot;) %&gt;% 
  rename(cpi = symbol,  # FRED data is given as &#39;symbol&#39; and &#39;price&#39;
         rate = price) %&gt;% # we rename them to what they really are, e.g., cpi and rate
  
  # calculate yearly change in CPI by dividing current month by same month a year (or 12 months) earlier, minus 1
  mutate(cpi_yoy_change = rate/lag(rate, 12) - 1)

ten_year_monthly  &lt;-   tq_get(&quot;GS10&quot;, get = &quot;economic.data&quot;,
                       from = &quot;1980-01-01&quot;) %&gt;% 
  rename(ten_year = symbol,
         yield = price) %&gt;% 
  mutate(yield = yield / 100) # original data is not given as, e.g., 0.05, but rather 5, for five percent

# we have the two dataframes-- we now need to join them, and we will use left_join()
# base R has a function merge() that does the same, but it&#39;s slow, so please don&#39;t use it

mydata &lt;- 
  cpi %&gt;% 
  left_join(ten_year_monthly, by=&quot;date&quot;) %&gt;% 
  mutate(
    year = year(date), # using lubridate::year() to generate a new column with just the year
    month = month(date, label = TRUE),
    decade=case_when(
      year %in% 1980:1989 ~ &quot;1980s&quot;,
      year %in% 1990:1999 ~ &quot;1990s&quot;,
      year %in% 2000:2009 ~ &quot;2000s&quot;,
      year %in% 2010:2019 ~ &quot;2010s&quot;,
      TRUE ~ &quot;2020s&quot;
      )
  )</code></pre>
<p>Now let’s reproduce the graph.</p>
<pre class="r"><code>ggplot(mydata,
       aes(x = cpi_yoy_change,
           y = yield,
           color = decade)) +
  geom_point() +
  geom_smooth(method = &#39;lm&#39;,    #create a trendline without confidence interval
              se = FALSE) +
  facet_wrap(~ decade,    #faceted by decade
             nrow = 5,
             scales = &quot;free&quot;)+
  ggrepel::geom_text_repel(aes(label = format(date,    #geom_text_repel to avoid overlap
                                              format = &quot;%b %Y&quot;)),
                           check_overlap = TRUE,
                           size = 2,
                           segment.color=&quot;transparent&quot;)+
  scale_x_continuous(labels = scales::percent)+    #x-axis as percentage
  scale_y_continuous(labels = scales::percent)+    #y-axis as percentage
  labs(x = &quot;CPI Yearly change&quot;,
       y = &quot;10-year Treasury Constant Maturity Rate&quot;,
       title = &quot;How are CPI and 10-year related?&quot;,
       caption = &quot;Data Source: FRED&quot;) +    #Add titles and a caption
  theme_bw()+    #Use classic theme
  theme(text = element_text(size = 12),
        legend.position = &quot;none&quot;,    #Remove legend
        aspect.ratio = 1/12) + #Set the ratio of each faceted graph&#39;s height and length
  NULL</code></pre>
<p><img src="/blogs/group_project_1_files/figure-html/unnamed-chunk-15-1.png" width="1728" style="display: block; margin: auto;" /></p>
</div>
<div id="challenge-2-opinion-polls-for-the-2021-german-elections" class="section level1">
<h1>Challenge 2: Opinion polls for the 2021 German elections</h1>
<p>The second challenge also deals with the reproduction of a sophisticated graph.</p>
<p>The Guardian newspaper has an <a href="https://www.theguardian.com/world/2021/aug/20/german-election-poll-tracker-who-will-be-the-next-chancellor">election poll tracker for the upcoming German election</a> and we will reproduce the graph of this tracker.</p>
<p>The following code will scrape the wikipedia page and import the table in a dataframe.</p>
<pre class="r"><code>url &lt;- &quot;https://en.wikipedia.org/wiki/Opinion_polling_for_the_2021_German_federal_election&quot;

# similar graphs and analyses can be found at 
# https://www.theguardian.com/world/2021/jun/21/german-election-poll-tracker-who-will-be-the-next-chancellor
# https://www.economist.com/graphic-detail/who-will-succeed-angela-merkel


# get tables that exist on wikipedia page 
tables &lt;- url %&gt;% 
  read_html() %&gt;% 
  html_nodes(css=&quot;table&quot;)

tables</code></pre>
<pre><code>## {xml_nodeset (20)}
##  [1] &lt;table class=&quot;wikitable sortable tpl-blanktable mw-collapsible&quot; style=&quot;t ...
##  [2] &lt;table class=&quot;wikitable sortable tpl-blanktable mw-collapsible mw-collap ...
##  [3] &lt;table class=&quot;wikitable sortable tpl-blanktable mw-collapsible mw-collap ...
##  [4] &lt;table class=&quot;wikitable sortable tpl-blanktable mw-collapsible mw-collap ...
##  [5] &lt;table class=&quot;wikitable sortable tpl-blanktable mw-collapsible mw-collap ...
##  [6] &lt;table class=&quot;wikitable sortable tpl-blanktable mw-collapsible mw-collap ...
##  [7] &lt;table class=&quot;wikitable sortable tpl-blanktable&quot; style=&quot;text-align:cente ...
##  [8] &lt;table class=&quot;wikitable sortable tpl-blanktable&quot; style=&quot;text-align:cente ...
##  [9] &lt;table class=&quot;wikitable sortable tpl-blanktable&quot; style=&quot;text-align:cente ...
## [10] &lt;table class=&quot;wikitable sortable tpl-blanktable&quot; style=&quot;text-align:cente ...
## [11] &lt;table class=&quot;wikitable sortable tpl-blanktable&quot; style=&quot;text-align:cente ...
## [12] &lt;table class=&quot;wikitable sortable tpl-blanktable&quot; style=&quot;text-align:cente ...
## [13] &lt;table class=&quot;wikitable sortable tpl-blanktable&quot; style=&quot;text-align:cente ...
## [14] &lt;table class=&quot;wikitable sortable tpl-blanktable&quot; style=&quot;text-align:cente ...
## [15] &lt;table class=&quot;wikitable sortable tpl-blanktable&quot; style=&quot;text-align:cente ...
## [16] &lt;table class=&quot;wikitable sortable tpl-blanktable&quot; style=&quot;text-align:cente ...
## [17] &lt;table class=&quot;wikitable sortable tpl-blanktable&quot; style=&quot;text-align:cente ...
## [18] &lt;table class=&quot;wikitable sortable tpl-blanktable&quot; style=&quot;text-align:cente ...
## [19] &lt;table class=&quot;wikitable sortable tpl-blanktable&quot; style=&quot;text-align:cente ...
## [20] &lt;table class=&quot;nowraplinks mw-collapsible autocollapse navbox-inner&quot; styl ...</code></pre>
<pre class="r"><code># parse HTML tables into a dataframe called polls 
# Use purr::map() to create a list of all tables in URL
polls &lt;- map(tables, . %&gt;% 
             html_table(fill=TRUE)%&gt;% 
             janitor::clean_names())

polls</code></pre>
<pre><code>## [[1]]
## # A tibble: 252 × 13
##    polling_firm    fieldwork_date samplesize abs   union   spd  af_d   fdp linke
##    &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 Polling firm    Fieldwork date Samplesize Abs.   NA    NA    NA    NA    NA  
##  2 2021 federal e… 26 Sep 2021    –          23.4   24.1  25.7  10.3  11.5   4.9
##  3 Wahlkreisprogn… 22–24 Sep 2021 1,400      –      22.5  25.5  11    12     7  
##  4 Ipsos           22–23 Sep 2021 2,000      –      22    26    11    12     7  
##  5 Forschungsgrup… 22–23 Sep 2021 1,273      –      23    25    10    11     6  
##  6 Forsa           20–23 Sep 2021 2,002      26     22    25    10    12     6  
##  7 Allensbach      16–23 Sep 2021 1,554      –      25    26    10    10.5   5  
##  8 Civey           16–23 Sep 2021 10,012     –      23    25    10    12     6  
##  9 YouGov          16–22 Sep 2021 2,364      –      21    25    12    11     7  
## 10 Wahlkreisprogn… 20–21 Sep 2021 1,801      –      21.5  25    11    12.5   6.5
## # … with 242 more rows, and 4 more variables: grune &lt;dbl&gt;, fw &lt;chr&gt;,
## #   others &lt;chr&gt;, lead &lt;chr&gt;
## 
## [[2]]
## # A tibble: 226 × 12
##    polling_firm    fieldwork_date samplesize abs   union   spd  af_d   fdp linke
##    &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 Polling firm    Fieldwork date Samplesize Abs.     NA  NA      NA    NA  NA  
##  2 Forsa           21–23 Dec 2020 1,501      –        36  15       9     6   9  
##  3 INSA            18–21 Dec 2020 2,055      –        35  16      11     8   7.5
##  4 Forsa           14–18 Dec 2020 2,505      23       37  15       8     6   8  
##  5 Kantar          10–16 Dec 2020 2,401      –        35  17      10     6   8  
##  6 INSA            11–14 Dec 2020 2,002      –        36  17      10     7   7.5
##  7 Forsa           7–11 Dec 2020  2,503      23       37  15       8     5   8  
##  8 Allensbach      28 Nov–10 Dec… 1,022      –        37  16.5     9     7   7  
##  9 Infratest dimap 7–9 Dec 2020   1,004      –        36  16       9     6   7  
## 10 Forschungsgrup… 7–9 Dec 2020   1,246      25       37  16      10     5   8  
## # … with 216 more rows, and 3 more variables: grune &lt;dbl&gt;, others &lt;chr&gt;,
## #   lead &lt;chr&gt;
## 
## [[3]]
## # A tibble: 234 × 12
##    polling_firm    fieldwork_date samplesize abs   union   spd  af_d   fdp linke
##    &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 Polling firm    Fieldwork date Samplesize Abs.     NA    NA    NA  NA    NA  
##  2 INSA            20–23 Dec 2019 2,034      –        28    13    15  10     8  
##  3 Forsa           16–20 Dec 2019 2,501      23       28    13    13   8     8  
##  4 Emnid           12–18 Dec 2019 1,899      –        27    15    14   9     9  
##  5 YouGov          13–17 Dec 2019 1,586      –        27    13    15   7    10  
##  6 INSA            13–16 Dec 2019 2,020      –        27    13    15   8.5   9.5
##  7 Forsa           9–13 Dec 2019  2,502      22       28    12    13   8     9  
##  8 Forschungsgrup… 10–12 Dec 2019 1,366      –        27    13    14   8     9  
##  9 Infratest dimap 10–11 Dec 2019 1,038      –        27    14    15   8     8  
## 10 Emnid           5–11 Dec 2019  1,978      –        28    16    13   9     9  
## # … with 224 more rows, and 3 more variables: grune &lt;dbl&gt;, others &lt;chr&gt;,
## #   lead &lt;chr&gt;
## 
## [[4]]
## # A tibble: 253 × 12
##    polling_firm    fieldwork_date samplesize abs   union   spd  af_d   fdp linke
##    &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 Polling firm    Fieldwork date Samplesize Abs.     NA    NA    NA  NA    NA  
##  2 YouGov          21–27 Dec 2018 1,665      –        30    16    14   8    10  
##  3 INSA            20–21 Dec 2018 1,029      –        29    14    15   9     9  
##  4 Forsa           17–21 Dec 2018 2,504      24       31    14    13   8     8  
##  5 Emnid           13–19 Dec 2018 1,889      –        29    15    14   9     9  
##  6 INSA            14–17 Dec 2018 2,077      –        29    15    15   9.5   9.5
##  7 Forsa           10–14 Dec 2018 2,507      24       32    15    12   8     8  
##  8 Forschungsgrup… 11–13 Dec 2018 1,268      –        30    15    15   7     9  
##  9 Infratest dimap 11–12 Dec 2018 1,048      –        31    15    13   8     8  
## 10 Emnid           6–12 Dec 2018  1,997      –        30    15    14   8     8  
## # … with 243 more rows, and 3 more variables: grune &lt;dbl&gt;, others &lt;chr&gt;,
## #   lead &lt;chr&gt;
## 
## [[5]]
## # A tibble: 68 × 12
##    polling_firm    fieldwork_date samplesize abs   union   spd  af_d   fdp linke
##    &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 Polling firm    Fieldwork date Samplesize Abs.     NA  NA      NA  NA      NA
##  2 YouGov          25–27 Dec 2017 1,489      –        33  21      13   9      10
##  3 INSA            21–22 Dec 2017 2,203      –        33  20.5    13  10.5    10
##  4 Forsa           18–22 Dec 2017 2,504      21       34  19      12   8      10
##  5 Emnid           14–20 Dec 2017 1,861      –        33  21      12   8      10
##  6 INSA            15–18 Dec 2017 2,031      –        31  21      14   9      11
##  7 Forsa           11–15 Dec 2017 2,501      21       33  20      12   8      10
##  8 Allensbach      1–14 Dec 2017  1,443      –        34  21      11  10       9
##  9 Infratest dimap 11–13 Dec 2017 1,029      –        32  20      13   9       9
## 10 Emnid           7–13 Dec 2017  1,865      –        32  22      13   8       9
## # … with 58 more rows, and 3 more variables: grune &lt;dbl&gt;, others &lt;chr&gt;,
## #   lead &lt;chr&gt;
## 
## [[6]]
## # A tibble: 212 × 12
##    polling_firm    fieldwork_date samplesize   cdu   spd  af_d   fdp linke grune
##    &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 Polling firm    Fieldwork date Samplesize  NA    NA    NA    NA    NA    NA  
##  2 2021 federal e… 26 Sep 2017    –           18.9  25.7  10.3  11.5   4.9  14.8
##  3 INSA            17–20 Sep 2021 2,054       17    25    11    12     6.5  15  
##  4 INSA            10–13 Sep 2021 2,062       16    26    11.5  12.5   6.5  15  
##  5 INSA            3–6 Sep 2021   2,052       15.5  26    11    12.5   6.5  15.5
##  6 INSA            27–30 Aug 2021 2,015       15    25    11    13.5   7    16.5
##  7 INSA            20–23 Aug 2021 2,119       18    23    11    13     7    17  
##  8 INSA            13–16 Aug 2021 2,080       20    20    11    12.5   6.5  17.5
##  9 INSA            6–9 Aug 2021   2,118       20.5  17.5  11.5  12.5   6.5  17.5
## 10 INSA            30 Jul–2 Aug … 2,080       22.5  18    11    13     7    18  
## # … with 202 more rows, and 3 more variables: csu &lt;dbl&gt;, others &lt;chr&gt;,
## #   lead &lt;chr&gt;
## 
## [[7]]
## # A tibble: 5 × 11
##   polling_firm    fieldwork_date  samplesize   cdu   spd grune   fdp  af_d linke
##   &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 Polling firm    Fieldwork date  Samplesize  NA    NA    NA    NA    NA    NA  
## 2 2021 federal e… 26 Sep 2021     –           24.8  21.6  17.2  15.3   9.6   3.3
## 3 Forsa           29 Jan – 1 Feb… 1,007       33    10    24    10    12     6  
## 4 Forsa           8–22 Feb 2018   1,003       35    13    17    11    12     8  
## 5 2017 federal e… 24 Sep 2017     –           34.4  16.4  13.5  12.7  12.2   6.4
## # … with 2 more variables: others &lt;chr&gt;, lead &lt;chr&gt;
## 
## [[8]]
## # A tibble: 25 × 12
##    polling_firm    fieldwork_date samplesize   csu   spd  af_d   fdp grune linke
##    &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 Polling firm    Fieldwork date Samplesize  NA      NA    NA  NA    NA    NA  
##  2 2021 federal e… 26 Sep 2021    –           31.7    18     9  10.5  14.1   2.8
##  3 GMS             13–15 Sep 2021 1,002       29      17    11  13    16     5  
##  4 GMS             8–13 Sep 2021  1,005       28      18    11  12    17     4  
##  5 Infratest dimap 2–6 Sep 2021   1,195       28      18    10  12    16     3  
##  6 GMS             1–6 Sep 2021   1,003       29      15    10  13    18     3  
##  7 GMS             21–27 Jul 2021 1,003       35       9     9  12    20     4  
##  8 INSA            12–19 Jul 2021 1,000       36      11     9   9    19     4  
##  9 Infratest dimap 2–6 Jul 2021   1,186       36       9    10  11    18     4  
## 10 GMS             16–21 Jun 2021 1,007       34      10     9  11    22     3  
## # … with 15 more rows, and 3 more variables: free_voters &lt;chr&gt;, others &lt;chr&gt;,
## #   lead &lt;chr&gt;
## 
## [[9]]
## # A tibble: 31 × 11
##    polling_firm   fieldwork_date  samplesize   cdu linke   spd grune  af_d   fdp
##    &lt;chr&gt;          &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 Polling firm   Fieldwork date  Samplesize  NA    NA    NA    NA    NA    NA  
##  2 2021 federal … 26 Sep 2021     –           15.9  11.4  23.5  22.4   8.4   8.1
##  3 Forsa          29 Jan – 6 Feb… 1,001       21    16    12    26    10     6  
##  4 Forsa          12–19 Dec 2019  1,005       21    16    13    24    10     7  
##  5 Forsa          21–26 Nov 2019  1,006       20    16    14    24    11     7  
##  6 Forsa          22–31 Oct 2019  1,002       22    15    13    24    11     6  
##  7 Forsa          17–26 Sep 2019  1,002       22    15    13    24    11     7  
##  8 Forsa          20–29 Aug 2019  1,003       22    17    14    24    10     7  
##  9 Forsa          17–25 Jul 2019  1,001       22    17    14    24    10     6  
## 10 Forsa          17–27 Jun 2019  1,004       21    16    14    24    11     7  
## # … with 21 more rows, and 2 more variables: others &lt;chr&gt;, lead &lt;chr&gt;
## 
## [[10]]
## # A tibble: 7 × 11
##   polling_firm     fieldwork_date samplesize   cdu  af_d   spd linke   fdp grune
##   &lt;chr&gt;            &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 Polling firm     Fieldwork date Samplesize  NA    NA    NA    NA    NA      NA
## 2 2021 federal el… 26 Sep 2021    –           15.3  18.1  29.5   8.5   9.3     9
## 3 Infratest dimap  25–28 Aug 2021 1,157       15    18    29    11     9       9
## 4 Forsa            10–15 Dec 2020 1,001       28    17    15    12     4      17
## 5 Forsa            17–20 Dec 2018 1,005       23    20    12    18     7      14
## 6 Forsa            7–9 Nov 2017   1,002       26    20    16    19     8       6
## 7 2017 federal el… 24 Sep 2017    –           26.7  20.2  17.6  17.2   7.1     5
## # … with 2 more variables: others &lt;chr&gt;, lead &lt;chr&gt;
## 
## [[11]]
## # A tibble: 9 × 11
##   polling_firm    fieldwork_date  samplesize   cdu   spd grune linke   fdp  af_d
##   &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 Polling firm    Fieldwork date  Samplesize  NA    NA    NA    NA    NA    NA  
## 2 2021 federal e… 26 Sep 2021     –           15.5  29.7  24.9   6.7  11.4   5  
## 3 Trend Research  13–16 Sep 2021  703         14    33    18     9    14     7  
## 4 Trend Research  30 Aug–2 Sep 2… 708         15    34    17    10    13     7  
## 5 Trend Research  12–18 Aug 2021  702         17    28    19     9    14     7  
## 6 Forsa           18 Dec 2019–6 … 1,069       21    16    29    11     8     9  
## 7 Universität Ha… 6 Jan–2 Mar 20… 1,069       20    24    32     8    10     3  
## 8 Forsa           27 Dec 2018–3 … 1,004       22    17    26    11    10     9  
## 9 2017 federal e… 24 Sep 2017     –           27.2  23.5  13.9  12.2  10.8   7.8
## # … with 2 more variables: others &lt;chr&gt;, lead &lt;chr&gt;
## 
## [[12]]
## # A tibble: 4 × 11
##   polling_firm     fieldwork_date samplesize   cdu   spd  af_d   fdp grune linke
##   &lt;chr&gt;            &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 Polling firm     Fieldwork date Samplesize  NA    NA    NA    NA    NA    NA  
## 2 2021 federal el… 26 Sep 2021    –           22.8  27.6   8.8  12.8  15.8   4.3
## 3 Forsa            8–22 Feb 2018  1,035       31    19    10    12    14    10  
## 4 2017 federal el… 24 Sep 2017    –           30.9  23.5  11.9  11.5   9.7   8.1
## # … with 2 more variables: others &lt;chr&gt;, lead &lt;chr&gt;
## 
## [[13]]
## # A tibble: 17 × 11
##    polling_firm    fieldwork_date samplesize   cdu  af_d linke   spd   fdp grune
##    &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 Polling firm    Fieldwork date Samplesize  NA    NA    NA    NA    NA    NA  
##  2 2021 federal e… 26 Sep 2021    –           17.4  18    11.1  29.1   8.2   7.8
##  3 INSA            10–16 Sep 2021 1,000       14    19    11    38     6     9  
##  4 Infratest dimap 2–7 Sep 2021   1,180       16    18    11    31     9     8  
##  5 Infratest dimap 19–24 Aug 2021 1,153       19    18    12    29     9     7  
##  6 INSA            6–12 Aug 2021  1,000       21    18    12    25     8    10  
##  7 INSA            16–22 Jul 2021 1,098       26    21    13    17     8    11  
##  8 Infratest dimap 8–13 Jul 2021  1,159       30    17    13    17     8     8  
##  9 Infratest dimap 12–18 May 2021 1,245       23    16    12    18     7    16  
## 10 Forsa           12–15 Jan 2021 1,002       32    15    16    12     5    13  
## 11 Infratest dimap 18–21 Nov 2020 1,000       34    15    12    20     4    10  
## 12 Forsa           2–10 Jan 2020  1,002       27    20    16     8     6    14  
## 13 Forsa           18–23 Sep 2019 1,002       28    21    14    10     6    14  
## 14 Forsa           4–11 Jan 2019  1,007       32    20    16    11     5    11  
## 15 Forsa           24–28 Jun 2018 1,002       30    21    19    12     5     8  
## 16 Forsa           2–11 Jan 2018  1,001       33    18    18    15     5     6  
## 17 2017 federal e… 24 Sep 2017    –           33.1  18.6  17.8  15.1   6.2   4.3
## # … with 2 more variables: others &lt;chr&gt;, lead &lt;chr&gt;
## 
## [[14]]
## # A tibble: 6 × 11
##   polling_firm     fieldwork_date samplesize   cdu   spd   fdp  af_d grune linke
##   &lt;chr&gt;            &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 Polling firm     Fieldwork date Samplesize  NA    NA    NA    NA    NA    NA  
## 2 2021 federal el… 26 Sep 2021    –           24.2  33.1  10.5   7.4  16.1   3.3
## 3 Forsa            19–28 May 2020 1,002       40    20     5     7    16     6  
## 4 Forsa            1–8 Feb 2019   1,010       33    19     8     9    19     7  
## 5 Forsa            8–22 Feb 2018  1,004       36    24     9     8    12     7  
## 6 2017 federal el… 24 Sep 2017    –           34.9  27.4   9.3   9.1   8.7   7  
## # … with 2 more variables: others &lt;chr&gt;, lead &lt;chr&gt;
## 
## [[15]]
## # A tibble: 7 × 11
##   polling_firm     fieldwork_date samplesize   cdu   spd   fdp  af_d grune linke
##   &lt;chr&gt;            &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 Polling firm     Fieldwork date Samplesize  NA    NA    NA    NA    NA    NA  
## 2 2021 federal el… 26 Sep 2021    –           26    29.1  11.4   7.3  16.1   3.7
## 3 Forsa            10–17 May 2021 1,058       24    18    11     8    28     4  
## 4 Forsa            Mar–Apr 2021   1,090       26    18    11     8    25     6  
## 5 Forsa            1–16 Aug 2019  1,505       27    17    10     9    25     6  
## 6 Forsa            8–22 Feb 2018  1,015       34    21    11     9    11     8  
## 7 2017 federal el… 24 Sep 2017    –           32.6  26    13.1   9.4   7.6   7.5
## # … with 2 more variables: others &lt;chr&gt;, lead &lt;chr&gt;
## 
## [[16]]
## # A tibble: 8 × 12
##   polling_firm     fieldwork_date samplesize   cdu   spd  af_d   fdp grune linke
##   &lt;chr&gt;            &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 Polling firm     Fieldwork date Samplesize  NA    NA    NA    NA    NA    NA  
## 2 2021 federal el… 26 Sep 2021    –           24.7  29.4   9.2  11.7  12.6   3.3
## 3 Infratest dimap  2–7 Sep 2021   1,160       23    30    10    11    13     4  
## 4 Infratest dimap  9–13 Jul 2021  1,153       30    22    11     9    17     4  
## 5 Forsa            1–8 Feb 2019   1,005       34    16    10    10    18     7  
## 6 Infratest dimap  5–7 Mar 2018   1,001       34    23    12     9    12     7  
## 7 Infratest dimap  8–12 Dec 2017  1,003       39    24     9     8    12     5  
## 8 2017 federal el… 24 Sep 2017    –           35.9  24.2  11.2  10.4   7.6   6.8
## # … with 3 more variables: free_voters &lt;chr&gt;, others &lt;chr&gt;, lead &lt;chr&gt;
## 
## [[17]]
## # A tibble: 6 × 11
##   polling_firm     fieldwork_date samplesize  af_d   cdu linke   spd   fdp grune
##   &lt;chr&gt;            &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 Polling firm     Fieldwork date Samplesize  NA    NA    NA    NA    NA    NA  
## 2 2021 federal el… 26 Sep 2021    –           24.6  17.2   9.3  19.3  11     8.6
## 3 INSA             6–13 Sep 2021  1,000       26    18    11    18    11     8  
## 4 Infratest dimap  13–18 Aug 2021 1,179       23    21    11    15    12    10  
## 5 INSA             2–9 Aug 2021   1,001       25    24    13    10    11    10  
## 6 2017 federal el… 24 Sep 2017    –           27    26.9  16.1  10.5   8.2   4.6
## # … with 2 more variables: others &lt;chr&gt;, lead &lt;chr&gt;
## 
## [[18]]
## # A tibble: 4 × 11
##   polling_firm     fieldwork_date samplesize   cdu  af_d linke   spd   fdp grune
##   &lt;chr&gt;            &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 Polling firm     Fieldwork date Samplesize  NA    NA    NA    NA    NA    NA  
## 2 2021 federal el… 26 Sep 2021    –           16.9  24    11.4  23.4   9     6.6
## 3 INSA             16–23 Aug 2021 1,000       18    22    18    21     9     5  
## 4 2017 federal el… 24 Sep 2017    –           28.8  22.7  16.9  13.2   7.8   4.1
## # … with 2 more variables: others &lt;chr&gt;, lead &lt;chr&gt;
## 
## [[19]]
## # A tibble: 69 × 25
##    polling_firm       fieldwork_date  samplesize spd_union spd_union_2 union_fdp
##    &lt;chr&gt;              &lt;chr&gt;           &lt;chr&gt;          &lt;int&gt;       &lt;int&gt; &lt;chr&gt;    
##  1 Polling firm       Fieldwork date  Samplesize        NA          NA &quot;&quot;       
##  2 Forschungsgruppe … 14–16 Sep 2021  1,406              9           9 &quot;8&quot;      
##  3 Forschungsgruppe … 7–9 Sep 2021    1,281              9           9 &quot;7&quot;      
##  4 Forschungsgruppe … 31 Aug–2 Sep 2… 1,301             12          12 &quot;8&quot;      
##  5 Forschungsgruppe … 24–26 Aug 2021  1,300             10          10 &quot;6&quot;      
##  6 Forschungsgruppe … 10–12 Aug 2021  1,252             12          12 &quot;10&quot;     
##  7 Forschungsgruppe … 27–29 Jul 2021  1,268             13          13 &quot;10&quot;     
##  8 Forschungsgruppe … 13–15 Jul 2021  1,224             13          13 &quot;11&quot;     
##  9 Forschungsgruppe … 22–24 Jun 2021  1,271             11          11 &quot;11&quot;     
## 10 Civey              8–9 Jun 2021    5,005              6           6 &quot;–&quot;      
## # … with 59 more rows, and 19 more variables: union_fdp_2 &lt;chr&gt;,
## #   spd_grune &lt;chr&gt;, spd_grune_2 &lt;chr&gt;, union_grune &lt;int&gt;, union_grune_2 &lt;int&gt;,
## #   spd_grune_linke &lt;int&gt;, spd_grune_linke_2 &lt;int&gt;, spd_grune_linke_3 &lt;int&gt;,
## #   union_grune_fdp &lt;int&gt;, union_grune_fdp_2 &lt;int&gt;, union_grune_fdp_3 &lt;int&gt;,
## #   spd_union_fdp &lt;chr&gt;, spd_union_fdp_2 &lt;chr&gt;, spd_union_fdp_3 &lt;chr&gt;,
## #   spd_grune_fdp &lt;chr&gt;, spd_grune_fdp_2 &lt;chr&gt;, spd_grune_fdp_3 &lt;chr&gt;,
## #   spdfdp &lt;chr&gt;, spdfdp_2 &lt;chr&gt;
## 
## [[20]]
## # A tibble: 3 × 2
##   mw_parser_output_navbar_display_inline… mw_parser_output_navbar_display_inlin…
##   &lt;chr&gt;                                   &lt;chr&gt;                                 
## 1 Federal                                 &quot;Federal\nCandidates\nMembers\nPolls\…
## 2 State                                   &quot;Baden-Württemberg\nBerlin\nMecklenbu…
## 3 Previous: 2020 — Next: 2022             &quot;Previous: 2020 — Next: 2022&quot;</code></pre>
<pre class="r"><code># list of opinion polls
german_election_polls &lt;- polls[[1]] %&gt;% # the first table on the page contains the list of all opinions polls
  slice(2:(n()-1)) %&gt;%  # drop the first row, as it contains again the variable names and last row that contains 2017 results
  mutate(
         # polls are shown to run from-to, e.g. 9-13 Aug 2021. We keep the last date, 13 Aug here, as the poll date
         # and we extract it by picking the last 11 characters from that field
         end_date = str_sub(fieldwork_date, -11),
         
         # end_date is still a string, so we convert it into a date object using lubridate::dmy()
         end_date = dmy(end_date),
         
         # we also get the month and week number from the date, if we want to do analysis by month- week, etc.
         month = month(end_date),
         week = isoweek(end_date)
         )</code></pre>
<pre class="r"><code>glimpse(german_election_polls) #check dataset</code></pre>
<pre><code>## Rows: 250
## Columns: 16
## $ polling_firm   &lt;chr&gt; &quot;2021 federal election&quot;, &quot;Wahlkreisprognose&quot;, &quot;Ipsos&quot;, …
## $ fieldwork_date &lt;chr&gt; &quot;26 Sep 2021&quot;, &quot;22–24 Sep 2021&quot;, &quot;22–23 Sep 2021&quot;, &quot;22–…
## $ samplesize     &lt;chr&gt; &quot;–&quot;, &quot;1,400&quot;, &quot;2,000&quot;, &quot;1,273&quot;, &quot;2,002&quot;, &quot;1,554&quot;, &quot;10,0…
## $ abs            &lt;chr&gt; &quot;23.4&quot;, &quot;–&quot;, &quot;–&quot;, &quot;–&quot;, &quot;26&quot;, &quot;–&quot;, &quot;–&quot;, &quot;–&quot;, &quot;–&quot;, &quot;–&quot;, &quot;…
## $ union          &lt;dbl&gt; 24.1, 22.5, 22.0, 23.0, 22.0, 25.0, 23.0, 21.0, 21.5, 2…
## $ spd            &lt;dbl&gt; 25.7, 25.5, 26.0, 25.0, 25.0, 26.0, 25.0, 25.0, 25.0, 2…
## $ af_d           &lt;dbl&gt; 10.3, 11.0, 11.0, 10.0, 10.0, 10.0, 10.0, 12.0, 11.0, 1…
## $ fdp            &lt;dbl&gt; 11.5, 12.0, 12.0, 11.0, 12.0, 10.5, 12.0, 11.0, 12.5, 1…
## $ linke          &lt;dbl&gt; 4.9, 7.0, 7.0, 6.0, 6.0, 5.0, 6.0, 7.0, 6.5, 7.0, 6.5, …
## $ grune          &lt;dbl&gt; 14.8, 14.0, 16.0, 16.5, 17.0, 16.0, 16.0, 14.0, 15.0, 1…
## $ fw             &lt;chr&gt; &quot;2.5&quot;, &quot;–&quot;, &quot;–&quot;, &quot;3&quot;, &quot;3&quot;, &quot;–&quot;, &quot;–&quot;, &quot;2&quot;, &quot;–&quot;, &quot;–&quot;, &quot;–&quot;…
## $ others         &lt;chr&gt; &quot;6.2&quot;, &quot;8&quot;, &quot;6&quot;, &quot;5.5&quot;, &quot;5&quot;, &quot;7.5&quot;, &quot;8&quot;, &quot;7&quot;, &quot;8.5&quot;, &quot;9…
## $ lead           &lt;chr&gt; &quot;1.6&quot;, &quot;3&quot;, &quot;4&quot;, &quot;2&quot;, &quot;3&quot;, &quot;1&quot;, &quot;2&quot;, &quot;4&quot;, &quot;3.5&quot;, &quot;4&quot;, &quot;…
## $ end_date       &lt;date&gt; 2021-09-26, 2021-09-24, 2021-09-23, 2021-09-23, 2021-0…
## $ month          &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9…
## $ week           &lt;dbl&gt; 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 37, 37,…</code></pre>
<p>Before plotting the graph, we first need to calculate rolling means for every party’s position in previous polls.</p>
<pre class="r"><code>#Calculate everyday means for every party&#39;s position
german_election_polls_mean &lt;- german_election_polls %&gt;%
  group_by(end_date) %&gt;% 
  summarise(mean_CDU = mean(union),
            mean_SPD = mean(spd),
            mean_Grune = mean(grune),
            mean_AfD = mean(af_d),
            mean_FDP = mean(fdp),
            mean_Linke = mean(linke))

#Then calculate 14-day rolling mean
german_election_polls_rollingmean &lt;- german_election_polls_mean %&gt;% 
  mutate(CDU14 = zoo::rollmean(mean_CDU,
                               k = 14,
                               fill = NA,
                               align = &quot;right&quot;),   #Make sure the calculation is based on 14 days BEFORE the date we examine instead of AFTER or BETWEEN
         SPD14 = zoo::rollmean(mean_SPD,
                               k = 14,
                               fill = NA,
                               align = &quot;right&quot;),
         Grune14 = zoo::rollmean(mean_Grune,
                               k = 14,
                               fill = NA,
                               align = &quot;right&quot;),
         AfD14 = zoo::rollmean(mean_AfD,
                               k = 14,
                               fill = NA,
                               align = &quot;right&quot;),
         FDP14 = zoo::rollmean(mean_FDP,
                               k = 14,
                               fill = NA,
                               align = &quot;right&quot;),
         Linke14 = zoo::rollmean(mean_Linke,
                               k = 14,
                               fill = NA,
                               align = &quot;right&quot;))

#Just to check if everything goes right
glimpse(german_election_polls_rollingmean)</code></pre>
<pre><code>## Rows: 146
## Columns: 13
## $ end_date   &lt;date&gt; 2021-01-04, 2021-01-05, 2021-01-06, 2021-01-08, 2021-01-11…
## $ mean_CDU   &lt;dbl&gt; 36.5, 36.0, 35.0, 36.0, 36.0, 36.0, 37.0, 35.0, 35.0, 35.2,…
## $ mean_SPD   &lt;dbl&gt; 15.5, 15.0, 14.0, 14.0, 15.0, 15.0, 15.0, 15.0, 15.0, 14.5,…
## $ mean_Grune &lt;dbl&gt; 18.0, 18.0, 21.0, 20.0, 18.0, 18.0, 20.0, 20.0, 19.0, 17.2,…
## $ mean_AfD   &lt;dbl&gt; 10.00, 10.00, 10.00, 8.00, 10.00, 10.00, 10.00, 9.00, 9.00,…
## $ mean_FDP   &lt;dbl&gt; 6.75, 6.00, 7.00, 7.00, 7.50, 7.00, 5.00, 6.00, 7.00, 8.75,…
## $ mean_Linke &lt;dbl&gt; 7.75, 9.00, 7.00, 8.00, 8.00, 8.00, 8.00, 8.00, 8.00, 7.75,…
## $ CDU14      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 35.9, 3…
## $ SPD14      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 14.9, 1…
## $ Grune14    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 18.9, 1…
## $ AfD14      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 9.61, 9…
## $ FDP14      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 6.85, 6…
## $ Linke14    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 7.85, 7…</code></pre>
<p>Rolling mean for the first 13 recorded days are <code>NA</code> because we have no previous data in this dataset. It does not hurt our graphing, so we can dismiss it this time.</p>
<pre class="r"><code>#Assign a color to each subset in preparation, try to replicate the color in the original graph as close as possible
color &lt;- c(&quot;CDU/CSU&quot; = &quot;black&quot;,
           &quot;SPD&quot; = &quot;red3&quot;,
           &quot;Grune&quot; = &quot;green3&quot;,
           &quot;AfD&quot; = &quot;steelblue4&quot;,
           &quot;FDP&quot; = &quot;goldenrod1&quot;,
           &quot;Linke&quot; = &quot;mediumorchid3&quot;)

ggplot(german_election_polls,
       aes(x = end_date)) +
  
  #Graph the point with some transparency
  geom_point(aes(y = union,
             color = &quot;CDU/CSU&quot;),
             alpha = 0.3,
             size = 2) +
  geom_point(aes(y = spd,
             color = &quot;SPD&quot;),
             alpha = 0.3,
             size = 2) +
  geom_point(aes(y = grune,
             color = &quot;Grune&quot;),
             alpha = 0.3,
             size = 2) +
  geom_point(aes(y = af_d,
             color = &quot;AfD&quot;),
             alpha = 0.3,
             size = 2) +
  geom_point(aes(y = fdp,
             color = &quot;FDP&quot;),
             alpha = 0.3,
             size = 2) +
  geom_point(aes(y = linke,
             color = &quot;Linke&quot;),
             alpha = 0.3,
             size = 2) +
  
  #Graph the line to show trend. Note we have no line for the first 13 sample points because it&#39;s based on 14-day rolling average
  geom_line(data = german_election_polls_rollingmean,
            aes(y = CDU14,
            color = &quot;CDU/CSU&quot;),
            size = 1.2) +
  geom_line(data = german_election_polls_rollingmean,
            aes(y = SPD14,
            color = &quot;SPD&quot;),
            size = 1.2) +
  geom_line(data = german_election_polls_rollingmean,
            aes(y = Grune14,
            color = &quot;Grune&quot;),
            size = 1.2) +
  geom_line(data = german_election_polls_rollingmean,
            aes(y = AfD14,
            color = &quot;AfD&quot;),
            size = 1.2) +
  geom_line(data = german_election_polls_rollingmean,
            aes(y = FDP14,
            color = &quot;FDP&quot;),
            size = 1.2) +
  geom_line(data = german_election_polls_rollingmean,
            aes(y = Linke14,
            color = &quot;Linke&quot;),
            size = 1.2) +
  
  #Set the format of the x-axis
  scale_x_date(date_breaks = &quot;1 month&quot;,
               date_labels = &quot;%b %Y&quot;) +
  
  #Set the limtis and break points of the y-axis
  scale_y_continuous(breaks = c(5,15,25,35,45)) +
  expand_limits(y = c(5,45)) +
  
  #Set color with pre-defined series
  scale_color_manual(name = &quot;Party&quot;,
                     values = color) +
  labs(x = &quot;Date&quot;,
       y = &quot;Share of Votes (%)&quot;,
       title = &quot;German election poll-tracker&quot;,
       subtitle = &quot;Who will be the next German chancellor?&quot;,
       caption = &quot;Source: Wikipedia, last updated on 26 Sep 2021&quot;) +
  theme(text = element_text(size = 14),
        legend.position = &quot;right&quot;,
        panel.background = element_rect(fill = &quot;white&quot;),
        panel.grid.major.x = element_blank(),   #Remove the x-axis grid
        panel.grid.minor.x = element_blank(),
        panel.grid.major.y = element_line(size = 0.5,    #Format the y-axis grid
                                          color = &quot;grey&quot;,
                                          linetype = &quot;dashed&quot; )) +
  NULL</code></pre>
<p><img src="/blogs/group_project_1_files/figure-html/unnamed-chunk-18-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="details" class="section level1">
<h1>Details</h1>
<ul>
<li>Who did you collaborate with: team members of Study Group A14</li>
<li>Approximately how much time did you spend on this problem set: 7 hours on average for each team member</li>
<li>What, if anything, gave you the most trouble: formatting the plots</li>
</ul>
<blockquote>
<p>As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?</p>
</blockquote>
<blockquote>
<p>Yes.</p>
</blockquote>
</div>
<div id="rubric" class="section level1">
<h1>Rubric</h1>
<p>Check minus (1/5): Displays minimal effort. Doesn’t complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn’t use plots appropriate for the variables being analyzed.</p>
<p>Check (3/5): Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output).</p>
<p>Check plus (5/5): Finished all components of the assignment correctly and addressed both challenges. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you’ve written additional text to describe how you interpret the output.</p>
</div>
