---
categories:  
- ""    #the front matter should be like the one found in, e.g., blog2.md. It cannot be like the normal Rmd we used
- ""
date: "2021-10-20"
description: Markdown output for 3rd group project # the title that will show up once someone gets to this page
draft: false
image: pic01.jpg # save picture in \static\img\blogs. Acceptable formats= jpg, jpeg, or png . Your iPhone pics wont work

keywords: ""
slug: group_project_3 # slug is the shorthand URL address... no spaces plz
title: Group Project 3
---



<div id="youth-risk-behavior-surveillance" class="section level1">
<h1>Youth Risk Behavior Surveillance</h1>
<p>Every two years, the Centers for Disease Control and Prevention conduct the <a href="https://www.cdc.gov/healthyyouth/data/yrbs/index.htm">Youth Risk Behavior Surveillance System (YRBSS)</a> survey, where it takes data from high schoolers (9th through 12th grade), to analyze health patterns. We will work with a selected group of variables from a random sample of observations during one of the years the YRBSS was conducted.</p>
<div id="load-the-data" class="section level2">
<h2>Load the data</h2>
<p>This data is part of the <code>openintro</code> textbook and we can load and inspect it. There are observations on 13 different variables, some categorical and some numerical.</p>
<pre class="r"><code>data(yrbss)
glimpse(yrbss)</code></pre>
<pre><code>## Rows: 13,583
## Columns: 13
## $ age                      &lt;int&gt; 14, 14, 15, 15, 15, 15, 15, 14, 15, 15, 15, 1…
## $ gender                   &lt;chr&gt; &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;fema…
## $ grade                    &lt;chr&gt; &quot;9&quot;, &quot;9&quot;, &quot;9&quot;, &quot;9&quot;, &quot;9&quot;, &quot;9&quot;, &quot;9&quot;, &quot;9&quot;, &quot;9&quot;, …
## $ hispanic                 &lt;chr&gt; &quot;not&quot;, &quot;not&quot;, &quot;hispanic&quot;, &quot;not&quot;, &quot;not&quot;, &quot;not&quot;…
## $ race                     &lt;chr&gt; &quot;Black or African American&quot;, &quot;Black or Africa…
## $ height                   &lt;dbl&gt; NA, NA, 1.73, 1.60, 1.50, 1.57, 1.65, 1.88, 1…
## $ weight                   &lt;dbl&gt; NA, NA, 84.4, 55.8, 46.7, 67.1, 131.5, 71.2, …
## $ helmet_12m               &lt;chr&gt; &quot;never&quot;, &quot;never&quot;, &quot;never&quot;, &quot;never&quot;, &quot;did not …
## $ text_while_driving_30d   &lt;chr&gt; &quot;0&quot;, NA, &quot;30&quot;, &quot;0&quot;, &quot;did not drive&quot;, &quot;did not…
## $ physically_active_7d     &lt;int&gt; 4, 2, 7, 0, 2, 1, 4, 4, 5, 0, 0, 0, 4, 7, 7, …
## $ hours_tv_per_school_day  &lt;chr&gt; &quot;5+&quot;, &quot;5+&quot;, &quot;5+&quot;, &quot;2&quot;, &quot;3&quot;, &quot;5+&quot;, &quot;5+&quot;, &quot;5+&quot;,…
## $ strength_training_7d     &lt;int&gt; 0, 0, 0, 0, 1, 0, 2, 0, 3, 0, 3, 0, 0, 7, 7, …
## $ school_night_hours_sleep &lt;chr&gt; &quot;8&quot;, &quot;6&quot;, &quot;&lt;5&quot;, &quot;6&quot;, &quot;9&quot;, &quot;8&quot;, &quot;9&quot;, &quot;6&quot;, &quot;&lt;5&quot;…</code></pre>
<p>Before we carry on with your analysis, it is always a good idea to check with <code>skimr::skim()</code> to get a feel for missing values, summary statistics of numerical variables, and a very rough histogram.</p>
<pre class="r"><code>#use skimr::skim() to get feel for missing values
skimr::skim(yrbss)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-2">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">yrbss</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">13583</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">13</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">8</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">5</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">gender</td>
<td align="right">12</td>
<td align="right">1.00</td>
<td align="right">4</td>
<td align="right">6</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">grade</td>
<td align="right">79</td>
<td align="right">0.99</td>
<td align="right">1</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="right">5</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">hispanic</td>
<td align="right">231</td>
<td align="right">0.98</td>
<td align="right">3</td>
<td align="right">8</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">race</td>
<td align="right">2805</td>
<td align="right">0.79</td>
<td align="right">5</td>
<td align="right">41</td>
<td align="right">0</td>
<td align="right">5</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">helmet_12m</td>
<td align="right">311</td>
<td align="right">0.98</td>
<td align="right">5</td>
<td align="right">12</td>
<td align="right">0</td>
<td align="right">6</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">text_while_driving_30d</td>
<td align="right">918</td>
<td align="right">0.93</td>
<td align="right">1</td>
<td align="right">13</td>
<td align="right">0</td>
<td align="right">8</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">hours_tv_per_school_day</td>
<td align="right">338</td>
<td align="right">0.98</td>
<td align="right">1</td>
<td align="right">12</td>
<td align="right">0</td>
<td align="right">7</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">school_night_hours_sleep</td>
<td align="right">1248</td>
<td align="right">0.91</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">7</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">age</td>
<td align="right">77</td>
<td align="right">0.99</td>
<td align="right">16.16</td>
<td align="right">1.26</td>
<td align="right">12.00</td>
<td align="right">15.0</td>
<td align="right">16.00</td>
<td align="right">17.00</td>
<td align="right">18.00</td>
<td align="left">▁▂▅▅▇</td>
</tr>
<tr class="even">
<td align="left">height</td>
<td align="right">1004</td>
<td align="right">0.93</td>
<td align="right">1.69</td>
<td align="right">0.10</td>
<td align="right">1.27</td>
<td align="right">1.6</td>
<td align="right">1.68</td>
<td align="right">1.78</td>
<td align="right">2.11</td>
<td align="left">▁▅▇▃▁</td>
</tr>
<tr class="odd">
<td align="left">weight</td>
<td align="right">1004</td>
<td align="right">0.93</td>
<td align="right">67.91</td>
<td align="right">16.90</td>
<td align="right">29.94</td>
<td align="right">56.2</td>
<td align="right">64.41</td>
<td align="right">76.20</td>
<td align="right">180.99</td>
<td align="left">▆▇▂▁▁</td>
</tr>
<tr class="even">
<td align="left">physically_active_7d</td>
<td align="right">273</td>
<td align="right">0.98</td>
<td align="right">3.90</td>
<td align="right">2.56</td>
<td align="right">0.00</td>
<td align="right">2.0</td>
<td align="right">4.00</td>
<td align="right">7.00</td>
<td align="right">7.00</td>
<td align="left">▆▂▅▃▇</td>
</tr>
<tr class="odd">
<td align="left">strength_training_7d</td>
<td align="right">1176</td>
<td align="right">0.91</td>
<td align="right">2.95</td>
<td align="right">2.58</td>
<td align="right">0.00</td>
<td align="right">0.0</td>
<td align="right">3.00</td>
<td align="right">5.00</td>
<td align="right">7.00</td>
<td align="left">▇▂▅▂▅</td>
</tr>
</tbody>
</table>
<p>From the above output, we can see that our dataset contains 13,583 observations and has 13 variables of which 8 are character and 5 are numeric variables. We can also see that for many of these variables we have missing values, the highest of which is the <code>race</code> of the high schoolers where we have 2,805 missing values.</p>
</div>
<div id="exploratory-data-analysis" class="section level2">
<h2>Exploratory Data Analysis</h2>
<p>We will first start with analyzing the <code>weight</code> of participants in kilograms. From the output above, we can already see that we have 1,004 missing values for the weights variable. We will first look at the distribution of weights of the high schoolers on a higher level and then look at it in more detail by grouping into gender and age.</p>
<pre class="r"><code>#summary statistics of weights of all high schoolers
yrbss %&gt;% 
dplyr::select(weight) %&gt;% # there are no instances where we there is no age data but there is weight data
  summarise(min_weight = min(weight, na.rm = TRUE), #set na.rm = TRUE to disregard observations with no weight information
  max_weight = max(weight, na.rm = TRUE),
  median_weight = median(weight, na.rm = TRUE),
  mean_weight = mean(weight, na.rm = TRUE),
  sd_weight = sd(weight, na.rm = TRUE),
  n = n()
  )</code></pre>
<pre><code>## # A tibble: 1 × 6
##   min_weight max_weight median_weight mean_weight sd_weight     n
##        &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;
## 1       29.9       181.          64.4        67.9      16.9 13583</code></pre>
<pre class="r"><code>#visualisation of distribution of weights of all high schoolers
ggplot(yrbss, aes(x=weight)) +
  geom_histogram() +
  labs(title = &quot;Distribution of weights of all high schoolers&quot;,
       x = &quot;weight&quot;,
       y = &quot;# of observations&quot;) +
  theme_bw()</code></pre>
<p><img src="/blogs/group_project_3_files/figure-html/eda_on_weight-1.png" width="648" style="display: block; margin: auto;" />
From the summary statistics above as well as the histogram we can see that our data is right-skewed, meaning that the mean is higher than the median. Therefore, it seems that we have many outliers on the higher end of the distribution that would be considered obese high schoolers.</p>
<p>Since the “normal” (or healthy) weight differs between boys and girls and also among age groups, we also want to look at the weight distribution in more detail for the different genders and for the different age groups. We do this with the following code.</p>
<pre class="r"><code>#first remove observations where gender or age is NA to have a clear summarising
yrbss_upd &lt;- yrbss %&gt;% 
  drop_na(age, gender)

#summary statistics for weight by age
yrbss_upd %&gt;% 
group_by(age) %&gt;% # there are no instances where we there is no age data but there is weight data
  summarise(min_weight = min(weight, na.rm = TRUE),
   max_weight = max(weight, na.rm = TRUE),
   median_weight = median(weight, na.rm = TRUE),
   mean_weight = mean(weight, na.rm = TRUE),
   sd_weight = sd(weight, na.rm = TRUE),
   count = n()
  )</code></pre>
<pre><code>## # A tibble: 7 × 7
##     age min_weight max_weight median_weight mean_weight sd_weight count
##   &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;
## 1    12       54.9       90.7          65.8        68.2      12.2    26
## 2    13       35.4      113.           62.6        65.3      21.4    18
## 3    14       31.8      159.           59.0        62.2      14.8  1368
## 4    15       29.9      146.           61.7        65.1      15.8  3097
## 5    16       33.1      163.           64.9        67.9      16.3  3202
## 6    17       38.6      159.           66          69.5      16.8  3471
## 7    18       34.0      181.           68.0        72.5      18.6  2319</code></pre>
<pre class="r"><code>#summary statistics for weight by gender
yrbss_upd %&gt;% 
group_by(gender) %&gt;% # there are no instances where we there is no age data but there is weight data
  summarise(min_weight = min(weight, na.rm = TRUE),
   max_weight = max(weight, na.rm = TRUE),
   median_weight = median(weight, na.rm = TRUE),
   mean_weight = mean(weight, na.rm = TRUE),
   sd_weight = sd(weight, na.rm = TRUE),
   count = n()
  )</code></pre>
<pre><code>## # A tibble: 2 × 7
##   gender min_weight max_weight median_weight mean_weight sd_weight count
##   &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;
## 1 female       29.9       148.          59.0        61.9      14.4  6594
## 2 male         31.8       181.          70.3        73.6      17.1  6907</code></pre>
<pre class="r"><code>#visualisation of distribution of weight by age group and gender
ggplot(yrbss_upd,aes(x = weight, fill = gender)) +
  geom_density()+
  facet_wrap(~ age, scales = &quot;free&quot;)+
  labs(title = &quot;Distribution of weight by age and gender&quot;,
       x = &quot;Weight&quot;,
       y = &quot;# of observations&quot;) +
  theme_bw() +
  #theme(legend.position = &quot;none&quot;) + 
  NULL</code></pre>
<p><img src="/blogs/group_project_3_files/figure-html/unnamed-chunk-4-1.png" width="648" style="display: block; margin: auto;" />
While we do not have a lot of data for 12- and 13-year olds, we can see from the density plots of the other ages that the weight distribution is very similar for the different ages. The data is always right-skewed with some outliers in the higher end. Comparing the different genders, we can see that the weight of male high schoolers is more variable and on average also higher than that of females.</p>
<p>Next, we consider the possible relationship between a high schooler’s weight and their physical activity. Plotting the data is a useful first step because it helps us quickly visualize trends, identify strong associations, and develop research questions. We do this with the help of density plots.</p>
<pre class="r"><code>#create densityplots to inspect relationship between physical activity and weight

yrbss %&gt;% 
  filter(!is.na(physically_active_7d)) %&gt;% 
  
ggplot(aes(x = weight)) +
  geom_density() +
  facet_wrap(~physically_active_7d, nrow = 7) + 
  labs(title = &quot;Distribution of weights per # of physically active days&quot;,
       x = NULL,
       y = &quot;Weight&quot;) + 
  theme_bw() +
  theme(axis.text.x = element_blank()) +
  NULL</code></pre>
<p><img src="/blogs/group_project_3_files/figure-html/unnamed-chunk-5-1.png" width="648" style="display: block; margin: auto;" />
After a first inspection of the data, we cannot really see a big difference in the weight of high schoolers that are less physically active compared to those that are. In order to further test the relationship, we create a new variable in the dataframe <code>yrbss</code>, called <code>physical_3plus</code> , which will be <code>yes</code> if they are physically active for at least 3 days a week, and <code>no</code> otherwise. We also calculate the number and % of those who are and are not active for more than 3 days. We first use the <code>count()</code> function and see if we get the same results as <code>group_by()... summarise()</code></p>
<pre class="r"><code>#create new variable physical_3plus
yrbss2 &lt;- yrbss %&gt;%
  #remove observations where no value for physicaly_active_7d
  filter(!is.na(physically_active_7d)) %&gt;% 
  mutate(physical_3plus = ifelse(physically_active_7d &gt;= 3, &quot;yes&quot;, &quot;no&quot;))

#calculate number of high schoolers in each group with count
yrbss2 %&gt;%
  count(physical_3plus) %&gt;% 
  mutate (perc = n/sum(n))</code></pre>
<pre><code>## # A tibble: 2 × 3
##   physical_3plus     n  perc
##   &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt;
## 1 no              4404 0.331
## 2 yes             8906 0.669</code></pre>
<pre class="r"><code>#calculate number and percentage with group_by and summarise
yrbss2 %&gt;% 
  group_by(physical_3plus) %&gt;% 
  summarise(n = n()) %&gt;% 
  mutate(perc = n/sum(n))</code></pre>
<pre><code>## # A tibble: 2 × 3
##   physical_3plus     n  perc
##   &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt;
## 1 no              4404 0.331
## 2 yes             8906 0.669</code></pre>
<pre class="r"><code>#look at summary statistics for weight of the two groups
favstats(weight~ physical_3plus, data = yrbss2)</code></pre>
<pre><code>##   physical_3plus  min   Q1 median   Q3 max mean   sd    n missing
## 1             no 29.9 54.4   62.6 74.8 181 66.7 17.6 4022     382
## 2            yes 33.1 56.7   65.8 77.1 160 68.4 16.5 8342     564</code></pre>
<p>Next, we provide a 95% confidence interval for the population proportion of high schoolers that are <em>NOT</em> active 3 or more days per week.</p>
<pre class="r"><code>yrbss2 %&gt;% 
  summarise(total = n(),
            perc_no = sum(physical_3plus == &quot;no&quot;)/total,
            SE = sqrt(perc_no * (1-perc_no)/total),
            z_critical = qt(0.975, total - 1),
            lower = perc_no - z_critical * SE,
            upper = perc_no + z_critical * SE
            )</code></pre>
<pre><code>## # A tibble: 1 × 6
##   total perc_no      SE z_critical lower upper
##   &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 13310   0.331 0.00408       1.96 0.323 0.339</code></pre>
<p>From the output above, we can see that the 95% confidence interval for the proportion of high schoolers being active 2 days or less per week is [0.323; 0.339].</p>
<p>To inspect the relationship between <code>physical_3plus</code> vs. <code>weight</code>, we next create a boxplot diagram.</p>
<pre class="r"><code># Boxplot of weights for those who answered yes or no 
yrbss2 %&gt;% 
  ggplot(aes(x = physical_3plus, y = weight))+
  geom_boxplot()+
  labs(title = &quot;Relationship between physical activity and weight of high schoolers&quot;,
       x = &quot;At least 3 physically active days per week&quot;,
       y = &quot;Weight (kg)&quot;) +
  theme_bw() +
  NULL</code></pre>
<p><img src="/blogs/group_project_3_files/figure-html/boxplot-1.png" width="648" style="display: block; margin: auto;" />
Somewhat surprisingly, those who exercise more than 3 times a week have a marginally higher median weight than those who exercise less than or equal to 3 times a week. Perhaps this is a result of the composition of those that do exercise more than 3 times a week, influential variables likely being age, gender and height. It might also be the case that those who excercise more have more muscles which are known to weigh more. Unsurprisingly, the highest observed weight was within the group that exercised less than 3 times a week. This was unsurprising because it is unlikely someone who exercises regularly, even considering height, age and gender, would weigh over 175kg. Considering this, the “no” group has the largest range possessing the lowest and highest weights of the entire data set. The two groups do, however, share similar inter-quartile ranges.</p>
</div>
<div id="confidence-interval" class="section level2">
<h2>Confidence Interval</h2>
<p>Boxplots show how the medians of the two distributions compare, but we can also compare the means of the distributions using either a confidence interval or a hypothesis test. Note that when we calculate the mean, SD, etc. weight in these groups using the mean function, we must ignore any missing values by setting the <code>na.rm = TRUE</code>.</p>
<pre class="r"><code>#calculate confidence intervals for the two groups
yrbss2 %&gt;% 
  group_by(physical_3plus) %&gt;% 
  summarise (mean_weight = mean(weight, na.rm = TRUE),
    sd_weight = sd(weight, na.rm = TRUE),
    count = n(),
    SE = sd_weight/sqrt(count),
    t_critical = qt(0.975, count - 1),
    lower = mean_weight - t_critical * SE,
    upper = mean_weight + t_critical * SE)</code></pre>
<pre><code>## # A tibble: 2 × 8
##   physical_3plus mean_weight sd_weight count    SE t_critical lower upper
##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 no                    66.7      17.6  4404 0.266       1.96  66.2  67.2
## 2 yes                   68.4      16.5  8906 0.175       1.96  68.1  68.8</code></pre>
<p>There is an observed difference of about 1.77kg (68.44 - 66.67), and we notice that the two confidence intervals do not overlap. It seems that the difference is at least 95% statistically significant. Let us also conduct a hypothesis test.</p>
</div>
<div id="hypothesis-test-with-formula" class="section level2">
<h2>Hypothesis test with formula</h2>
<p>For our hypothesis test, we first write down the null and alternative hypotheses. The null hypothesis assumes that there is no difference in the mean weight between high schoolers who are physically active at least 3 times a week and those who are not. The alternative hypothesis states that there is a difference.</p>
<p><span class="math inline">\(Claim (null \space hypothesis) \space H_0: \delta = 0\)</span></p>
<p><span class="math inline">\(Altnerative \space hypothesis \space H_a: \delta ≠ 0\)</span></p>
<pre class="r"><code>t.test(weight ~ physical_3plus, data = yrbss2)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  weight by physical_3plus
## t = -5, df = 7479, p-value = 9e-08
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -2.42 -1.12
## sample estimates:
##  mean in group no mean in group yes 
##              66.7              68.4</code></pre>
<p>Looking at the output from the <code>t.test()</code> above, we can see that the p-value is very small, and the confidence interval does not contain 0. Therefore, we can reject the null hypothesis based on this output.</p>
</div>
<div id="hypothesis-test-with-infer" class="section level2">
<h2>Hypothesis test with <code>infer</code></h2>
<p>Next, we will introduce a new function, <code>hypothesize</code>, that falls into the infer workflow. We will use this method for conducting hypothesis tests.</p>
<p>But first, we need to initialize the test, which we will save as <code>obs_diff</code>.</p>
<pre class="r"><code>obs_diff &lt;- yrbss2 %&gt;%
  specify(weight ~ physical_3plus) %&gt;%
  calculate(stat = &quot;diff in means&quot;, order = c(&quot;yes&quot;, &quot;no&quot;))</code></pre>
<p>Notice how we can use the functions <code>specify</code> and <code>calculate</code> again like we did for calculating confidence intervals. Here, though, the statistic we are searching for is the difference in means, with the order being yes - no != 0.</p>
<p>After we have initialized the test, we need to simulate the test on the null distribution, which we will save as null.</p>
<pre class="r"><code>set.seed(112)

null_dist &lt;- yrbss2 %&gt;%
  # specify variables
  specify(weight ~ physical_3plus) %&gt;%
  
  # assume independence, i.e, there is no difference
  hypothesize(null = &quot;independence&quot;) %&gt;%
  
  # generate 1000 reps, of type &quot;permute&quot;
  generate(reps = 1000, type = &quot;permute&quot;) %&gt;%
  
  # calculate statistic of difference, namely &quot;diff in means&quot;
  calculate(stat = &quot;diff in means&quot;, order = c(&quot;yes&quot;, &quot;no&quot;))</code></pre>
<p>Here, <code>hypothesize</code> is used to set the null hypothesis as a test for independence, i.e., that there is no difference between the two population means. In one sample cases, the null argument can be set to <em>point</em> to test a hypothesis relative to a point estimate.</p>
<p>Also, note that the <code>type</code> argument within <code>generate</code> is set to <code>permute</code>, which is the argument when generating a null distribution for a hypothesis test.</p>
<p>We can visualize this null distribution with the following code:</p>
<pre class="r"><code>ggplot(data = null_dist, aes(x = stat)) +
  geom_histogram()</code></pre>
<p><img src="/blogs/group_project_3_files/figure-html/unnamed-chunk-7-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>Now that the test is initialized and the null distribution formed, we can visualise to see how many of these null permutations have a difference of at least <code>obs_stat</code> of 1.77.</p>
<p>We can also calculate the p-value for your hypothesis test using the function <code>infer::get_p_value()</code>.</p>
<pre class="r"><code>null_dist %&gt;% visualize() +
  shade_p_value(obs_stat = obs_diff, direction = &quot;two-sided&quot;)</code></pre>
<p><img src="/blogs/group_project_3_files/figure-html/unnamed-chunk-8-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>null_dist %&gt;%
  get_p_value(obs_stat = obs_diff, direction = &quot;two_sided&quot;)</code></pre>
<pre><code>## # A tibble: 1 × 1
##   p_value
##     &lt;dbl&gt;
## 1       0</code></pre>
<p>This the standard workflow for performing hypothesis tests. We have got the same result (rejection of null hypothesis) as in doing hypothesis test with <code>t.test()</code> function.</p>
</div>
</div>
<div id="imdb-ratings-differences-between-directors" class="section level1">
<h1>IMDB ratings: Differences between directors</h1>
<p>Let us recall the IMBD ratings data from the past homeworks. This time, we want to explore whether the mean IMDB rating for Steven Spielberg and Tim Burton are the same or not. Looking at the graph below, where the the confidence intervals for the mean ratings of these two directors have been calculated, we can see they overlap.</p>
<p><img src="/Users/matthewtlane/Documents/LBS/CA09 Data Analytics for Finance/Lecture 1/Git Repo/my_website/images/directors.png" width="100%" style="display: block; margin: auto;" /></p>
<p>As a first step, we want to reproduce this graph.</p>
<p>In addition, we will run a hypothesis test using both the <code>t.test</code> command and the <code>infer</code> package to simulate from a null distribution, where we assume zero difference between the two.</p>
<blockquote>
<p>Before anything, we write down the null and alternative hypotheses.</p>
</blockquote>
<p>We set up a null hypothesis that assumes that there is no difference in means, i.e. that the difference is 0 and the effect is not real. Our alternative hypothesis is that the difference is not 0. In mathematical terms, this leads to the following two hypotheses:</p>
<p><span class="math inline">\(Claim (null \space hypothesis) \space H_0: \delta = 0\)</span></p>
<p><span class="math inline">\(Altnerative \space hypothesis \space H_a: \delta ≠ 0\)</span></p>
<p>We also choose our significance level at 5%.</p>
<p>Now we can load the data and examine its structure.</p>
<pre class="r"><code>movies &lt;- read_csv(here::here(&quot;data&quot;, &quot;movies.csv&quot;))
glimpse(movies)</code></pre>
<pre><code>## Rows: 2,961
## Columns: 11
## $ title               &lt;chr&gt; &quot;Avatar&quot;, &quot;Titanic&quot;, &quot;Jurassic World&quot;, &quot;The Avenge…
## $ genre               &lt;chr&gt; &quot;Action&quot;, &quot;Drama&quot;, &quot;Action&quot;, &quot;Action&quot;, &quot;Action&quot;, &quot;…
## $ director            &lt;chr&gt; &quot;James Cameron&quot;, &quot;James Cameron&quot;, &quot;Colin Trevorrow…
## $ year                &lt;dbl&gt; 2009, 1997, 2015, 2012, 2008, 1999, 1977, 2015, 20…
## $ duration            &lt;dbl&gt; 178, 194, 124, 173, 152, 136, 125, 141, 164, 93, 1…
## $ gross               &lt;dbl&gt; 7.61e+08, 6.59e+08, 6.52e+08, 6.23e+08, 5.33e+08, …
## $ budget              &lt;dbl&gt; 2.37e+08, 2.00e+08, 1.50e+08, 2.20e+08, 1.85e+08, …
## $ cast_facebook_likes &lt;dbl&gt; 4834, 45223, 8458, 87697, 57802, 37723, 13485, 920…
## $ votes               &lt;dbl&gt; 886204, 793059, 418214, 995415, 1676169, 534658, 9…
## $ reviews             &lt;dbl&gt; 3777, 2843, 1934, 2425, 5312, 3917, 1752, 1752, 35…
## $ rating              &lt;dbl&gt; 7.9, 7.7, 7.0, 8.1, 9.0, 6.5, 8.7, 7.5, 8.5, 7.2, …</code></pre>
<p>In order to reproduce the graph above, we will first modify the <code>movies</code> dataset.</p>
<pre class="r"><code>#prepare dataset
directors &lt;- movies %&gt;% 
  filter(director  %in% c(&quot;Tim Burton&quot;, &quot;Steven Spielberg&quot;)) #only keep observations with Burton and Spielberg as directors

#calculate CI of mean ratings with formula
directors_CI &lt;- directors %&gt;% 
  group_by(director) %&gt;% 
  summarise(
    mean_rating = mean(rating),
    n = count(director),
    SE = sd(rating)/sqrt(n),
    t_critical = qt(0.975, (n-1)),
    lower = mean_rating - t_critical * SE,
    upper = mean_rating + t_critical * SE
  ) 

directors_CI</code></pre>
<pre><code>## # A tibble: 2 × 7
##   director         mean_rating     n    SE t_critical lower upper
##   &lt;chr&gt;                  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 Steven Spielberg        7.57    23 0.145       2.07  7.27  7.87
## 2 Tim Burton              6.93    16 0.187       2.13  6.53  7.33</code></pre>
<p>Now that we have calculated the confidence intervals for the ratings of the two directors, we can plot the graph from above.</p>
<pre class="r"><code>ggplot(directors_CI, aes(x = mean_rating,
                      y = fct_reorder(director, mean_rating),
                      color = director)) +
  
  #add rectangle first so that appears in background
  geom_rect(xmin = 7.27, xmax = 7.33,
            ymin = 0, ymax = Inf,
            linetype = &quot;blank&quot;,
            fill = &quot;grey&quot;,
            alpha = 0.5) +
  
  #add point and bar showing CIs
  geom_point(size = 5) + 
  geom_errorbarh(aes(xmin = lower, #add bars to visualise CI
                    xmax = upper),
                 size = 2,
                 height = 0.1) +
  labs(title = &quot;Do Spielberg and Burton have the same mean IMDB ratings?&quot;,
       subtitle = &quot;95% confidence intervals overlap&quot;,
       x = &quot;Mean IMBD Rating&quot;,
       y = NULL,
       color = NULL) +
  
  #add labels to CIs
  annotate(&quot;text&quot;, x = 7.57, y = 2.13, label = &quot;7.57&quot;, size = 8) +
  annotate(&quot;text&quot;, x = 7.27, y = 2.12, label = &quot;7.27&quot;, size = 5) +
  annotate(&quot;text&quot;, x = 7.87, y = 2.12, label = &quot;7.87&quot;, size = 5) +
  annotate(&quot;text&quot;, x = 6.93, y = 1.13, label = &quot;6.93&quot;, size = 8) +
  annotate(&quot;text&quot;, x = 6.53, y = 1.12, label = &quot;6.53&quot;, size = 5) +
  annotate(&quot;text&quot;, x = 7.33, y = 1.12, label = &quot;7.33&quot;, size = 5) +
  theme_bw() + 
  theme(plot.title = element_text(size = 13, face = &quot;bold&quot;),
        text = element_text(size = 11),
        legend.position = &quot;none&quot;) +
  NULL</code></pre>
<p><img src="/blogs/group_project_3_files/figure-html/unnamed-chunk-10-1.png" width="960" style="display: block; margin: auto;" />
Since the confidence intervals shown above overlap, we will run a hypothesis test in order to check whether there is a significant difference in means. We will do this once with the <code>t.test()</code> functions and the other time with the help of the <code>infer</code> package.</p>
<pre class="r"><code>#hypothesis test using t.test()
t.test(rating ~ director, data = directors)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  rating by director
## t = 3, df = 31, p-value = 0.01
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.16 1.13
## sample estimates:
## mean in group Steven Spielberg       mean in group Tim Burton 
##                           7.57                           6.93</code></pre>
<pre class="r"><code>#hypothesis test using simulation
set.seed(113)

#calculate observed difference in means
observed_difference &lt;- directors %&gt;%
  specify(rating ~ director) %&gt;%
  calculate(stat = &quot;diff in means&quot;, order = c(&quot;Steven Spielberg&quot;, &quot;Tim Burton&quot;))

#simulate data under a scenario where difference is 0
ratings_comp &lt;- directors %&gt;% 
  specify(rating ~ director) %&gt;% 
  hypothesise(null = &quot;independence&quot;) %&gt;% 
  generate(reps = 1000, type = &quot;permute&quot;) %&gt;% 
  calculate(stat = &quot;diff in means&quot;,
            order = c(&quot;Steven Spielberg&quot;, &quot;Tim Burton&quot;))

#visualise distribution of null hypothesis world and show observed difference as red line
ratings_comp %&gt;% visualize() +
  shade_p_value(obs_stat = observed_difference, direction = &quot;two-sided&quot;)</code></pre>
<p><img src="/blogs/group_project_3_files/figure-html/unnamed-chunk-12-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#calculate p-value
ratings_comp %&gt;% 
  get_pvalue(obs_stat = observed_difference,
             direction = &quot;both&quot;)</code></pre>
<pre><code>## # A tibble: 1 × 1
##   p_value
##     &lt;dbl&gt;
## 1   0.002</code></pre>
<blockquote>
<p>Resulting test statistic and the associated t-stat or p-value and conclusion</p>
</blockquote>
<p>Looking at the results of <code>t.test</code> and the simulation exercise, we reject the null hypothesis. <code>t.test</code> gives us a t-statistic of 3 and a p-value of 0.01, which is significantly below our pre-set significance level. Looking at the resulting confidence interval of [0.16, 1.13] from the <code>t.test</code> function, we can also see that 0 does not lie in this interval. This result is confirmed by the hypothesis test done with the help of the <code>infer</code> package, which results in a p-value of 0.002, which is also well below our significance level of 0.05. Therefore, we conclude that the movies by Steven Spielberg and Tim Burton do not have the same IMDB rating, with Steven Spielberg achieving higher ratings.</p>
</div>
<div id="omega-group-plc--pay-discrimination" class="section level1">
<h1>Omega Group plc- Pay Discrimination</h1>
<p>At the last board meeting of Omega Group Plc., the headquarters of a large multinational company, the issue was raised that women were being discriminated in the company, in the sense that the salaries were not the same for male and female executives. A quick analysis of a sample of 50 employees (of which 24 men and 26 women) revealed that the average salary for men was about 8,700 higher than for women. This seemed like a considerable difference, so it was decided that a further analysis of the company salaries was warranted.</p>
<p>The objective of this analysis is to find out whether there is indeed a significant difference between the salaries of men and women, and whether the difference is due to discrimination or whether it is based on another, possibly valid, determining factor.</p>
<div id="loading-the-data" class="section level2">
<h2>Loading the data</h2>
<pre class="r"><code>omega &lt;- read_csv(here::here(&quot;data&quot;, &quot;omega.csv&quot;))

# examine the data frame
glimpse(omega) </code></pre>
<pre><code>## Rows: 50
## Columns: 3
## $ salary     &lt;dbl&gt; 81894, 69517, 68589, 74881, 65598, 76840, 78800, 70033, 635…
## $ gender     &lt;chr&gt; &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;ma…
## $ experience &lt;dbl&gt; 16, 25, 15, 33, 16, 19, 32, 34, 1, 44, 7, 14, 33, 19, 24, 3…</code></pre>
</div>
<div id="relationship-salary---gender" class="section level2">
<h2>Relationship Salary - Gender ?</h2>
<p>The data frame <code>omega</code> contains the salaries for the sample of 50 executives in the company. We want to answer the question whether there is a significant difference between the salaries of the male and female executives.</p>
<p>We can perform different types of analyses, and check whether they all lead to the same conclusion</p>
<ul>
<li>Confidence intervals</li>
<li>Hypothesis testing</li>
<li>Correlation analysis</li>
<li>Regression</li>
</ul>
<p>As a first step, we calculate summary statistics on salary by gender. Also, we create and print a dataframe where, for each gender, we show the mean, SD, sample size, the t-critical, the SE, the margin of error, and the low/high endpoints of a 95% confidence interval</p>
<pre class="r"><code># Summary Statistics of salary by gender
mosaic::favstats (salary ~ gender, data=omega)</code></pre>
<pre><code>##   gender   min    Q1 median    Q3   max  mean   sd  n missing
## 1 female 47033 60338  64618 70033 78800 64543 7567 26       0
## 2   male 54768 68331  74675 78568 84576 73239 7463 24       0</code></pre>
<pre class="r"><code># Dataframe with two rows (male-female) and having as columns gender, mean, SD, sample size, 
# the t-critical value, the standard error, the margin of error, 
# and the low/high endpoints of a 95% confidence interval
salary_by_gender &lt;- omega %&gt;%
  group_by(gender) %&gt;% 
  summarise(mean_salary = mean(salary),
            sd_salary = sd(salary),
            n = n(),
            t_crit = qt(0.975, n-1),
            SE = sd_salary/sqrt(n),
            margin_of_error = t_crit * SE,
            lower = mean_salary - margin_of_error,
            upper = mean_salary + margin_of_error
  )

salary_by_gender</code></pre>
<pre><code>## # A tibble: 2 × 9
##   gender mean_salary sd_salary     n t_crit    SE margin_of_error  lower  upper
##   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1 female      64543.     7567.    26   2.06 1484.           3056. 61486. 67599.
## 2 male        73239.     7463.    24   2.07 1523.           3151. 70088. 76390.</code></pre>
<blockquote>
<p>Conclusion from above analysis</p>
</blockquote>
<p>The data analysis shows that there is indeed a difference in the
salaries of men and women. The difference in the mean and median of men and women is almost 10,000 even though the dispersion of the data sets is very similar as shown by their standard deviations. The most interesting part is that the 95% confidence intervals do not overlap at all, meaning that we can be 95% confident that the true means do not overlap and we can reject the null hypothesis that there is no difference in the salary means of men and women.</p>
<p>We can also run a hypothesis testing, assuming as a null hypothesis that the mean difference in salaries is zero, or that, on average, men and women make the same amount of money. We run our hypothesis testing using <code>t.test()</code> and with the simulation method from the <code>infer</code> package. In addition, we calculate the correlation coefficient between gender and salary and perform a linear regression with gender as indepedent and salary as dependent variable.</p>
<pre class="r"><code># hypothesis testing using t.test() 
t.test(salary ~ gender, data=omega)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  salary by gender
## t = -4, df = 48, p-value = 2e-04
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -12973  -4420
## sample estimates:
## mean in group female   mean in group male 
##                64543                73239</code></pre>
<pre class="r"><code># hypothesis testing using infer package
test_diff &lt;- omega %&gt;%
  specify(salary ~ gender) %&gt;%
  hypothesize(null = &quot;independence&quot;) %&gt;%
  generate(reps =1000, type =&quot;permute&quot;) %&gt;%
  calculate(stat = &quot;diff in means&quot;,
            order = c(&quot;male&quot;, &quot;female&quot;))

#calculate difference in means to calculate mean value
salary_diff &lt;- omega %&gt;%
  specify(salary ~ gender) %&gt;%
  calculate(stat = &quot;diff in means&quot;, order = c(&quot;male&quot;, &quot;female&quot;))

test_diff %&gt;% 
  get_pvalue(obs_stat = salary_diff,
             direction = &quot;both&quot;)</code></pre>
<pre><code>## # A tibble: 1 × 1
##   p_value
##     &lt;dbl&gt;
## 1       0</code></pre>
<p>As a next step, we also calculate the correlation coefficient between gender and salary and perform a linear regression.</p>
<pre class="r"><code>#calculate correlation coefficient between gender and salary
# use function biserial.cor from ltm package
library(ltm)
biserial.cor(omega$salary, omega$gender, level = 2)</code></pre>
<pre><code>## [1] 0.508</code></pre>
<pre class="r"><code>#alternative method - assign number to gender and calculate coefficient with cor() function
omega %&gt;% 
  mutate(genderN = ifelse(gender == &quot;male&quot;, 1, 0)) %&gt;% 
  dplyr::select(genderN, salary) %&gt;% 
  cor()</code></pre>
<pre><code>##         genderN salary
## genderN   1.000  0.508
## salary    0.508  1.000</code></pre>
<pre class="r"><code>#run linear regression with salary as dependent variable and gender as independent variable
salary_reg &lt;- glm(salary ~ gender, data = omega)
summary(salary_reg) #significant result</code></pre>
<pre><code>## 
## Call:
## glm(formula = salary ~ gender, data = omega)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -18471   -4780     127    5484   14257  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    64543       1474   43.78  &lt; 2e-16 ***
## gendermale      8696       2128    4.09  0.00017 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 56509713)
## 
##     Null deviance: 3656271994  on 49  degrees of freedom
## Residual deviance: 2712466235  on 48  degrees of freedom
## AIC: 1038
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<blockquote>
<p>Conclusion from the analysis above</p>
</blockquote>
<p>The correlation between gender and salary was found by assigning the male gender a value of 1. The correlation was then found to be 0.508, which means there is a moderate positive correlation between salary and gender.
Both hypothesis tests also show that there is a significant difference in the salaries of men and women as the p value is 0, meaning that the null hypothesis, which states that the mean differences of the salaries of men and women is 0, can be rejected.</p>
</div>
<div id="relationship-experience---gender" class="section level2">
<h2>Relationship Experience - Gender?</h2>
<p>At the board meeting, someone raised the issue that there was indeed a substantial difference between male and female salaries, but that this was attributable to other reasons such as differences in experience. A questionnaire send out to the 50 executives in the sample reveals that the average experience of the men is approximately 21 years, whereas the women only have about 7 years experience on average (see table below).</p>
<pre class="r"><code># Summary Statistics of experience by gender
favstats (experience ~ gender, data=omega)</code></pre>
<pre><code>##   gender min    Q1 median   Q3 max  mean    sd  n missing
## 1 female   0  0.25    3.0 14.0  29  7.38  8.51 26       0
## 2   male   1 15.75   19.5 31.2  44 21.12 10.92 24       0</code></pre>
<p>Based on this evidence, as the data shows above, we can conclude that there is a huge difference in the experience of male and female executives. The mean and median for male executives is 21.1 and 19.5 respectively while the mean and median for females is 3.0 and 7.4 respectively. The difference in the mean of the two groups is 13.7 and the difference in the median of the two groups is 16.5. While there is big difference in the mean and median, the standard deviations are fairly similar meaning the spread of the datasets is somewhat similar.</p>
<p>We now perform similar analyses as in the previous section and will test whether theses analyses validate or endanger our conclusion about the difference in male and female salaries.</p>
<pre class="r"><code># Dataframe with two rows (male-female) and having as columns gender, mean, SD, sample size, 
# the t-critical value, the standard error, the margin of error, 
# and the low/high endpoints of a 95% confidence interval
experience_by_gender &lt;- omega %&gt;%
  group_by(gender) %&gt;% 
  summarise(mean_experience = mean(experience),
            sd_experience = sd(experience),
            n = n(),
            t_crit = qt(0.975, n-1),
            SE = sd_experience/sqrt(n),
            margin_of_error = t_crit * SE,
            lower = mean_experience - margin_of_error,
            upper = mean_experience + margin_of_error
  )

experience_by_gender</code></pre>
<pre><code>## # A tibble: 2 × 9
##   gender mean_experience sd_experience     n t_crit    SE margin_of_error lower
##   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt; &lt;dbl&gt;
## 1 female            7.38          8.51    26   2.06  1.67            3.44  3.95
## 2 male             21.1          10.9     24   2.07  2.23            4.61 16.5 
## # … with 1 more variable: upper &lt;dbl&gt;</code></pre>
<p>Let us know also run the <code>t.test()</code> and a simulation with <code>infer</code>.</p>
<pre class="r"><code># hypothesis testing using t.test() 
t.test(experience ~ gender, data=omega)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  experience by gender
## t = -5, df = 43, p-value = 1e-05
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -19.35  -8.13
## sample estimates:
## mean in group female   mean in group male 
##                 7.38                21.12</code></pre>
<pre class="r"><code># hypothesis testing using infer package
diff_experience &lt;- omega %&gt;%
  specify(experience ~ gender) %&gt;%
  hypothesize(null = &quot;independence&quot;) %&gt;%
  generate(reps =1000, type =&quot;permute&quot;) %&gt;%
  calculate(stat = &quot;diff in means&quot;,
            order = c(&quot;female&quot;, &quot;male&quot;))

#calculate difference in means to calculate mean value
experience_diff &lt;- omega %&gt;%
  specify(experience ~ gender) %&gt;%
  calculate(stat = &quot;diff in means&quot;, order = c(&quot;male&quot;, &quot;female&quot;))


diff_experience %&gt;% 
  get_pvalue(obs_stat = experience_diff,
             direction = &quot;both&quot;)</code></pre>
<pre><code>## # A tibble: 1 × 1
##   p_value
##     &lt;dbl&gt;
## 1       0</code></pre>
<pre class="r"><code>#calculate correlation coefficient between gender and experience
# use function biserial.cor from ltm package
biserial.cor(omega$experience, omega$gender, level = 2)</code></pre>
<pre><code>## [1] 0.584</code></pre>
<pre class="r"><code>#alternative method - assign number to gender and calculate coefficient with cor() function
omega %&gt;% 
  mutate(genderN = ifelse(gender == &quot;male&quot;, 1, 0)) %&gt;% 
  dplyr::select(genderN, experience) %&gt;% 
  cor()</code></pre>
<pre><code>##            genderN experience
## genderN      1.000      0.584
## experience   0.584      1.000</code></pre>
<pre class="r"><code>#run linear regression with salary as dependent variable and gender as independent variable
experience_reg &lt;- glm(experience ~ gender, data = omega)
summary(experience_reg) #significant result</code></pre>
<pre><code>## 
## Call:
## glm(formula = experience ~ gender, data = omega)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -20.12   -6.32   -2.25    8.37   22.88  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     7.38       1.91    3.87  0.00033 ***
## gendermale     13.74       2.76    4.98  8.5e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 94.8)
## 
##     Null deviance: 6909.0  on 49  degrees of freedom
## Residual deviance: 4552.8  on 48  degrees of freedom
## AIC: 373.5
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<p>This results of our analysis above endangers our conclusion that the difference in salaries is only due to the gender of the employees. As can be seen from the two hypothesis tests as well as the correlation coefficient and the linear regression (showing that the gender male has significantly more experience), there is a difference in male and female experiences. The t test of experience and gender returned a p value of 0, meaning that the null hypothesis can be rejected. This indicates that experience might also be a reason that leads to the difference in salaries, and thus the difference in salaries of men and women is not only due to gender.</p>
</div>
<div id="relationship-salary---experience" class="section level2">
<h2>Relationship Salary - Experience ?</h2>
<p>Someone at the meeting argues that clearly, a more thorough analysis of the relationship between salary and experience is required before any conclusion can be drawn about whether there is any gender-based salary discrimination in the company.</p>
<p>We now analyse the relationship between salary and experience. First, we draw a scatterplot to visually inspect the data.</p>
<pre class="r"><code>#draw scatterplot to show relationship between salary and experience
ggplot(omega, aes(x=experience, y = salary))+
  geom_point() +
  geom_smooth(se=FALSE) +
  theme_bw() + 
  labs(title = &quot;Relationship between experience and salary&quot;,
          y = &quot;Salary&quot;,
          x = &quot;Experience (years)&quot;) +
  NULL</code></pre>
<p><img src="/blogs/group_project_3_files/figure-html/salary_exp_scatter-1.png" width="648" style="display: block; margin: auto;" />
Now that we see a positive relation between salary and experience in the scatterplot, we can also calculate the correlation coefficient.</p>
<pre class="r"><code>#calculate correlation coefficient
omega %&gt;% 
  dplyr::select(salary, experience) %&gt;% 
  cor()</code></pre>
<pre><code>##            salary experience
## salary      1.000      0.803
## experience  0.803      1.000</code></pre>
<pre class="r"><code>#perform linear regression
experience_salary_reg &lt;- lm(salary ~ experience, data = omega)
summary(experience_salary_reg)</code></pre>
<pre><code>## 
## Call:
## lm(formula = salary ~ experience, data = omega)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -13516  -2926    634   2952  11997 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  60549.0     1142.6   52.99  &lt; 2e-16 ***
## experience     584.3       62.6    9.34  2.3e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5200 on 48 degrees of freedom
## Multiple R-squared:  0.645,  Adjusted R-squared:  0.638 
## F-statistic: 87.2 on 1 and 48 DF,  p-value: 2.26e-12</code></pre>
<p>The scatterplot between salary and experience shows that there is a positive correlation between the two variables. This is supported by the correlation coefficient, which is calculated to be 0.8, which means the two variables are strongly positively correlated to one another. While correlation does not equal causation, this still is a strong sign that salaries are based on the experience of the employees. This is indeed confirmed by the linear regression we perform which shows that the salary increases by 584.3 for each 1-year increase in experience. The result is statistically significant.</p>
</div>
<div id="check-correlations-between-the-data" class="section level2">
<h2>Check correlations between the data</h2>
<p>We can use <code>GGally:ggpairs()</code> to create a scatterplot and correlation matrix. Essentially, we change the order our variables will appear in and have the dependent variable (Y), salary, as last in our list. We then pipe the dataframe to <code>ggpairs()</code> with <code>aes</code> arguments to colour by <code>gender</code> and make the plots somewhat transparent (<code>alpha  = 0.3</code>).</p>
<pre class="r"><code>omega %&gt;% 
  dplyr::select(gender, experience, salary) %&gt;% #order variables they will appear in ggpairs()
  ggpairs(aes(colour=gender, alpha = 0.3))+
  theme_bw()</code></pre>
<p><img src="/blogs/group_project_3_files/figure-html/ggpairs-1.png" width="648" style="display: block; margin: auto;" /></p>
<blockquote>
<p>Conclusion from salary vs experience scatterplot</p>
</blockquote>
<p>The salary vs experience scatterplot infers that there is a positive relationship between the two, meaning that the higher the salary, the higher you can expect the experience to be and vice versa. The scatterplot also shows us the dots colored according to the gender and we can clearly see that the blue dots representing male employees are more to the upper-right whereas the red dots representing female employees are mainly concentrated in the bottom left corner. This shows that male employees have a higher experience than female employees and are also generally paid more. However, looking at the red dot that is the furthest to the right (around 30 years of experience) and comparing it to a similar blue dot, the salary of this employee seems to be rather low compared to that of male employees with a similar level of experience. The same holds true for those employees with experiences between 10 and 20 years where the highest salaries are all paid to men. Therefore, there might still be a certain level of discrimination based on gender. However, more data would be needed in order to verify this statement.</p>
</div>
</div>
<div id="challenge-1-yield-curve-inversion" class="section level1">
<h1>Challenge 1: Yield Curve inversion</h1>
<p>Every so often, we hear warnings from commentators on the “inverted yield curve” and its predictive power with respect to recessions. An explainer what a <a href="https://www.reuters.com/article/us-usa-economy-yieldcurve-explainer/explainer-what-is-an-inverted-yield-curve-idUSKBN1O50GA">inverted yield curve is can be found here</a> or in a great podcast from <a href="https://www.podbean.com/media/share/dir-4zgj9-6aefd11">NPR on yield curve indicators</a>. A very nice article that explains the <a href="https://fredblog.stlouisfed.org/2018/10/the-data-behind-the-fear-of-yield-curve-inversions/">yield curve is and its inversion can be found here</a></p>
<p>In addition, many articles and commentators think that, e.g., <a href="https://www.bloomberg.com/news/articles/2019-08-14/u-k-yield-curve-inverts-for-first-time-since-financial-crisis"><em>Yield curve inversion is viewed as a harbinger of recession</em></a>. One can always doubt whether inversions are truly a harbinger of recessions, and <a href="https://twitter.com/5_min_macro/status/1161627360946511873">use the attached parable on yield curve inversions</a>.</p>
<p><img src="/Users/matthewtlane/Documents/LBS/CA09 Data Analytics for Finance/Lecture 1/Git Repo/my_website/images/yield_curve_parable.jpg" width="70%" style="display: block; margin: auto;" /></p>
<p>In order to better understand yield curve inversion, we will look at US data and use the <a href="https://fred.stlouisfed.org/">FRED database</a> to download historical yield curve rates, and plot the yield curves since 1999 to see when the yield curves flatten and invert. At the end of this challenge we will produce this chart:</p>
<p><img src="/Users/matthewtlane/Documents/LBS/CA09 Data Analytics for Finance/Lecture 1/Git Repo/my_website/images/yield_curve_challenge.png" width="100%" style="display: block; margin: auto;" /></p>
<p>First, we will load the yield curve data file that contains data on the yield curve since 1960-01-01.</p>
<pre class="r"><code>yield_curve &lt;- read_csv(here::here(&quot;data&quot;, &quot;yield_curve.csv&quot;))

glimpse(yield_curve)</code></pre>
<pre><code>## Rows: 6,884
## Columns: 5
## $ date      &lt;date&gt; 1960-01-01, 1960-02-01, 1960-03-01, 1960-04-01, 1960-05-01,…
## $ series_id &lt;chr&gt; &quot;TB3MS&quot;, &quot;TB3MS&quot;, &quot;TB3MS&quot;, &quot;TB3MS&quot;, &quot;TB3MS&quot;, &quot;TB3MS&quot;, &quot;TB3MS…
## $ value     &lt;dbl&gt; 4.35, 3.96, 3.31, 3.23, 3.29, 2.46, 2.30, 2.30, 2.48, 2.30, …
## $ maturity  &lt;chr&gt; &quot;3m&quot;, &quot;3m&quot;, &quot;3m&quot;, &quot;3m&quot;, &quot;3m&quot;, &quot;3m&quot;, &quot;3m&quot;, &quot;3m&quot;, &quot;3m&quot;, &quot;3m&quot;, …
## $ duration  &lt;chr&gt; &quot;3-Month Treasury Bill&quot;, &quot;3-Month Treasury Bill&quot;, &quot;3-Month T…</code></pre>
<p>Our dataframe <code>yield_curve</code> has five columns (variables):</p>
<ul>
<li><code>date</code>: already a date object</li>
<li><code>series_id</code>: the FRED database ticker symbol</li>
<li><code>value</code>: the actual yield on that date</li>
<li><code>maturity</code>: a short hand for the maturity of the bond</li>
<li><code>duration</code>: the duration, written out in all its glory!</li>
</ul>
<div id="plotting-the-yield-curve" class="section level2">
<h2>Plotting the yield curve</h2>
<p>We will produce three plots to see what yield curves look like since 1960. We will firstly look at the given plot and then try to produce our own version.</p>
<div id="yields-on-us-rates-by-duration-since-1960" class="section level3">
<h3>Yields on US rates by duration since 1960</h3>
<p>Given plot:</p>
<p><img src="/Users/matthewtlane/Documents/LBS/CA09 Data Analytics for Finance/Lecture 1/Git Repo/my_website/images/yield_curve1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Produce our own plot:</p>
<pre class="r"><code>#use factor() to convert duration into a factor (with order)
yield_curve &lt;- yield_curve %&gt;% 
  mutate(duration = factor(duration,
                           levels = c(&quot;3-Month Treasury Bill&quot;,
                                      &quot;6-Month Treasury Bill&quot;,
                                      &quot;1-Year Treasury Rate&quot;,
                                      &quot;2-Year Treasury Rate&quot;,
                                      &quot;3-Year Treasury Rate&quot;,
                                      &quot;5-Year Treasury Rate&quot;,
                                      &quot;7-Year Treasury Rate&quot;,
                                      &quot;10-Year Treasury Rate&quot;,
                                      &quot;20-Year Treasury Rate&quot;,
                                      &quot;30-Year Treasury Rate&quot;)))

#create the line plot
ggplot(yield_curve,
       aes(x = date,
           y = value,
           color = duration)) +
  geom_line() +
  facet_wrap(~duration,
             nrow = 5) +
  labs(x = NULL,
       y = &quot;%&quot;,
       title = &quot;Yields on U.S. Treasury rates since 1960&quot;,
       caption = &quot;Source: St. Louis Federal Reserve Economic Database (FRED)&quot;) +
  theme_bw() +
  theme(text = element_text(size = 11),
        legend.position = &quot;none&quot;) +
  NULL</code></pre>
<p><img src="/blogs/group_project_3_files/figure-html/unnamed-chunk-18-1.png" width="1152" style="display: block; margin: auto;" /></p>
</div>
<div id="monthly-yields-on-us-rates-by-duration-since-1999-on-a-year-by-year-basis" class="section level3">
<h3>Monthly yields on US rates by duration since 1999 on a year-by-year basis</h3>
<p>Given plot:</p>
<p><img src="/Users/matthewtlane/Documents/LBS/CA09 Data Analytics for Finance/Lecture 1/Git Repo/my_website/images/yield_curve2.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Produce our own plot:</p>
<p>Before we do the plotting, we need to add two new columns to explicitly record the year and month of every date.</p>
<pre class="r"><code>#use factor() to convert maturity into a factor (with order)
#label the year and month of every sample point
yield_curve &lt;- yield_curve %&gt;% 
  mutate(maturity = factor(maturity,
                           levels = c(&quot;3m&quot;, &quot;6m&quot;, &quot;1y&quot;, &quot;2y&quot;, &quot;3y&quot;, &quot;5y&quot;, &quot;7y&quot;, &quot;10y&quot;, &quot;20y&quot;, &quot;30y&quot;)),
         year = year(date),
         month = month(date, label = TRUE))

glimpse(yield_curve)</code></pre>
<pre><code>## Rows: 6,884
## Columns: 7
## $ date      &lt;date&gt; 1960-01-01, 1960-02-01, 1960-03-01, 1960-04-01, 1960-05-01,…
## $ series_id &lt;chr&gt; &quot;TB3MS&quot;, &quot;TB3MS&quot;, &quot;TB3MS&quot;, &quot;TB3MS&quot;, &quot;TB3MS&quot;, &quot;TB3MS&quot;, &quot;TB3MS…
## $ value     &lt;dbl&gt; 4.35, 3.96, 3.31, 3.23, 3.29, 2.46, 2.30, 2.30, 2.48, 2.30, …
## $ maturity  &lt;fct&gt; 3m, 3m, 3m, 3m, 3m, 3m, 3m, 3m, 3m, 3m, 3m, 3m, 3m, 3m, 3m, …
## $ duration  &lt;fct&gt; 3-Month Treasury Bill, 3-Month Treasury Bill, 3-Month Treasu…
## $ year      &lt;dbl&gt; 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, …
## $ month     &lt;ord&gt; Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec, …</code></pre>
<p>Now we can proceed with plotting.</p>
<pre class="r"><code>#create the line plot
#use factor() to prevent the color pool from becoming continuous
yield_curve %&gt;% 
  filter(year &gt;= 1999) %&gt;% 
  ggplot(aes(x = maturity,
             y = value,
             color = factor(year))) +
  
  #use group argument to group data by month in each facet
  geom_line(aes(group = month)) +
  
  facet_wrap(~year,
             nrow = 6) +
  labs(x = &quot;Maturity&quot;,
       y = &quot;Yield(%)&quot;,
       title = &quot;US Yield Curve&quot;,
       caption = &quot;Source: St. Lous Federal Reserve Economic Database (FRED)&quot;) +
  theme_bw() +
  theme(text = element_text(size = 11),
        legend.position = &quot;none&quot;) +
  NULL</code></pre>
<p><img src="/blogs/group_project_3_files/figure-html/unnamed-chunk-20-1.png" width="1152" style="display: block; margin: auto;" /></p>
</div>
<div id="month-and-10-year-yields-since-1999" class="section level3">
<h3>3-month and 10-year yields since 1999</h3>
<p>Given plot:</p>
<p><img src="/Users/matthewtlane/Documents/LBS/CA09 Data Analytics for Finance/Lecture 1/Git Repo/my_website/images/yield_curve3.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Produce our own plot:</p>
<pre class="r"><code>yield_curve %&gt;% 
  
  #filter by multiple conditions
  filter(maturity == &quot;3m&quot; | maturity == &quot;10y&quot;,
         year &gt;= 1999) %&gt;% 
  
  #create line plot
  ggplot(aes(x = date,
             y = value,
             color = duration)) +
  geom_line() +
  labs(x = NULL,
       y = &quot;%&quot;,
       title = &quot;Yields on 3-month and 10-year US Treasury rates since 1999&quot;,
       caption = &quot;Source: St. Lous Federal Reserve Economic Database (FRED)&quot;) +
  theme_bw() +
  theme(text = element_text(size = 11),
        legend.title = element_blank()) +
  NULL</code></pre>
<p><img src="/blogs/group_project_3_files/figure-html/unnamed-chunk-21-1.png" width="1152" style="display: block; margin: auto;" /></p>
</div>
<div id="plotting-yield-curve-inversion-and-recession" class="section level3">
<h3>Plotting yield curve inversion and recession</h3>
<p>First let’s use the graph above to see how yield curve inversion predicted recession since 1999.</p>
<p>According to <a href="https://en.wikipedia.org/wiki/List_of_recessions_in_the_United_States">Wikipedia’s list of recession in the United States</a>, since 1999 there have been two recession in the US: between Mar 2001–Nov 2001 and between Dec 2007–June 2009. Looking at the graphs above, we investigate whether the yield curve seems to flatten before these recessions and whether a yield curve flattening really means a recession is coming in the US</p>
<ul>
<li><p>We see from the second graph that the yield curve flattened in 2000 and 2006-2007, perfectly preceding the two recessions mentioned. So, yes the yield curve does appear to flatten before these recessions.</p></li>
<li><p>Although, the yield curve flattening appears to be a leading indicator of a recession we cannot conclude on the casualty of this relationship. It could merely be a coincidental correlation. However, due to the repetition and predictability of this relationship one could likely draw inference if it occurs again in the future. The likely justification for the relationship is that the inversion is actually a somewhat binary reflection of the current macro-economic landscape and so produces a clear output from the various economic, political and societal inputs leading up to it. It may prove more predictive if the “signal” is combined with further economic analysis of the US and global economies.</p></li>
<li><p>Short-term (3 months) yield exceeded longer term (10 years) yield briefly in late 2000, late 2006 and late 2019 all signalling an upcoming recession.</p></li>
</ul>
<p>To generalize this phenomena, we need to expand our time horizon and make a more informative graph indicating when the recession arrived.</p>
<p>The code below creates a dataframe with all US recessions since 1946.</p>
<pre class="r"><code># get US recession dates after 1946 from Wikipedia 
# https://en.wikipedia.org/wiki/List_of_recessions_in_the_United_States

recessions &lt;- tibble(
  from = c(&quot;1948-11-01&quot;, &quot;1953-07-01&quot;, &quot;1957-08-01&quot;, &quot;1960-04-01&quot;, &quot;1969-12-01&quot;, &quot;1973-11-01&quot;, &quot;1980-01-01&quot;,&quot;1981-07-01&quot;, &quot;1990-07-01&quot;, &quot;2001-03-01&quot;, &quot;2007-12-01&quot;,&quot;2020-02-01&quot;),  
  to = c(&quot;1949-10-01&quot;, &quot;1954-05-01&quot;, &quot;1958-04-01&quot;, &quot;1961-02-01&quot;, &quot;1970-11-01&quot;, &quot;1975-03-01&quot;, &quot;1980-07-01&quot;, &quot;1982-11-01&quot;, &quot;1991-03-01&quot;, &quot;2001-11-01&quot;, &quot;2009-06-01&quot;, &quot;2020-04-30&quot;) 
  )  %&gt;% 
  mutate(from = ymd(from), 
         to = ymd(to),
         duration_days = to - from)

head(recessions)</code></pre>
<pre><code>## # A tibble: 6 × 3
##   from       to         duration_days
##   &lt;date&gt;     &lt;date&gt;     &lt;drtn&gt;       
## 1 1948-11-01 1949-10-01 334 days     
## 2 1953-07-01 1954-05-01 304 days     
## 3 1957-08-01 1958-04-01 243 days     
## 4 1960-04-01 1961-02-01 306 days     
## 5 1969-12-01 1970-11-01 335 days     
## 6 1973-11-01 1975-03-01 485 days</code></pre>
<p>We will first calculate the spread between 10-year and 3-month treasury yields and then seperately mark the positive and negative spread into two new columns.</p>
<pre class="r"><code>yield_curve_spread &lt;- yield_curve %&gt;% 
  
  #select only relevant data
  dplyr::select(date, value, maturity) %&gt;% 
  
  #choose only 3-month and 10-year yields
  filter(maturity == &quot;3m&quot; | maturity == &quot;10y&quot;) %&gt;% 
  
  #pivot the table to wider form with maturity as new names
  pivot_wider(names_from = maturity,
              values_from = value) %&gt;% 
  
  #calculate yield spread, need to use ` to recognize column names with number
  mutate(spread = `10y` - `3m`,
         positive_spread = pmax(spread, 0),
         negative_spread = pmin(spread, 0))


glimpse(yield_curve_spread)</code></pre>
<pre><code>## Rows: 740
## Columns: 6
## $ date            &lt;date&gt; 1960-01-01, 1960-02-01, 1960-03-01, 1960-04-01, 1960-…
## $ `3m`            &lt;dbl&gt; 4.35, 3.96, 3.31, 3.23, 3.29, 2.46, 2.30, 2.30, 2.48, …
## $ `10y`           &lt;dbl&gt; 4.72, 4.49, 4.25, 4.28, 4.35, 4.15, 3.90, 3.80, 3.80, …
## $ spread          &lt;dbl&gt; 0.37, 0.53, 0.94, 1.05, 1.06, 1.69, 1.60, 1.50, 1.32, …
## $ positive_spread &lt;dbl&gt; 0.37, 0.53, 0.94, 1.05, 1.06, 1.69, 1.60, 1.50, 1.32, …
## $ negative_spread &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …</code></pre>
<p>Now we can reproduce the graph:</p>
<pre class="r"><code>ggplot(yield_curve_spread,
       aes(x = date)) +
  
  #add line plot for yield spreads and a horizontal line for zero spread
  geom_line(aes(y = spread)) +
  geom_hline(yintercept = 0,
             color = &quot;black&quot;,
             size = 0.6) +
  
  #use geom_ribbon to see whether spread is positive or negative
  geom_ribbon(aes(ymin = 0, ymax = positive_spread),
              fill = &quot;dodgerblue3&quot;,
              alpha = 0.3) +
  geom_ribbon(aes(ymin = negative_spread, ymax = 0),
              fill = &quot;red3&quot;,
              alpha = 0.3) +
  
  #use geom_rect to add the grey shaded areas corresponding to recessions
  #geom_rect doesn&#39;t produce the desired effect in terms of alpha, so we use annotate(geom = &quot;rect&quot;, ...) to optimize the result
  ggplot2::annotate(geom = &quot;rect&quot;,
           xmin = recessions$from, xmax = recessions$to,
           ymin = -Inf, ymax = Inf,
           fill = &quot;black&quot;,
           alpha = 0.2) +
  
  #use geom_rug for the small indicators at the bottom, with two differently sliced dataset from yield_curve_spread
  geom_rug(data = yield_curve_spread[yield_curve_spread[ , &quot;positive_spread&quot;] != 0, , drop = FALSE],
           color = &quot;dodgerblue3&quot;,
           alpha = 0.5,
           sides = &quot;b&quot;) +
  geom_rug(data = yield_curve_spread[yield_curve_spread[ , &quot;negative_spread&quot;] != 0, , drop = FALSE],
           color = &quot;red3&quot;,
           alpha = 0.5,
           sides = &quot;b&quot;) +
  
  #set the format of x-axis
  scale_x_date(date_labels = &quot;%Y&quot;,
               limits = c(date(&quot;1959-01-01&quot;), date(&quot;2023-01-01&quot;)),
               expand = c(0.03, 0.03),
               date_breaks = &quot;2 years&quot;,
               date_minor_breaks = &quot;1 year&quot;) +
  
  #some other formatting
  labs(x = NULL,
       y = &quot;Difference (10 year - 3 month) yield in %&quot;,
       title = &quot;Yield Curve Inversion: 10-year minus 3-month U.S. Treasury rates&quot;,
       subtitle = &quot;Difference in % points, monthly averages\nShaded areas correspond to recessions&quot;,
       caption = &quot;Source: FRED, Federal Reserve Bank at St. Louis&quot;) +
  theme_bw() +
  theme(text = element_text(size = 9),
        plot.title = element_text(face = &quot;bold&quot;),
        plot.subtitle = element_text(face = &quot;italic&quot;),
        panel.border = element_blank(),
        panel.grid = element_line(color = &quot;grey95&quot;),
        axis.ticks = element_blank()) +
  NULL</code></pre>
<p><img src="/blogs/group_project_3_files/figure-html/unnamed-chunk-23-1.png" width="1728" style="display: block; margin: auto;" /></p>
<p>Our answer for whether yield curve inversion can predict recession is: roughly yes. Among all 8 yield curve inversions since 1960, 7 of them were closely followed by an economic recession in US, and among all 8 recessions since 1960, also 7 of them were preceded by a yield curve inversion, while the remaining one (Jul 1990 - Mar 1991) was preceded by a deep yield curve flattening where the spread between 10-year and 3-month US Treasury rates almost hit zero.</p>
<ul>
<li><p>One explanation for such relationship is expectations hypothesis: long-term rates are the expected short-term rate in the future. Under this hypothesis, yield on 10-year Treasury bond can be interpreted as the market’s expectation on future short-term rate. When the market expect a rate cut from the Federal Reserve in the future in order to boost weak economy, this expectation will be reflected on the declining long-term yield. A self-fulfilling prophecy will pull down long-term yield further, eventually to a level below short-term yield.</p></li>
<li><p>Another explanation is, before recession actually arrives at the economy, investors who foresee such downside risk would rush to buy long-term bonds to secure their future cash flow as a safe haven asset, thus pushing up the price of long-term bonds, reducing their yields. When more and more people realize how bad the economic outlook is, long-term yield will go further down until a yield curve inversion happens.</p></li>
</ul>
<p>In conclusion, if a yield curve inversion happens today, it is highly likely that a recession is on the way. We don’t have a certain causality at this point, but the repeated sequences of historic yield curve inversions and recessions have convinced us that these two things are correlated.</p>
</div>
</div>
</div>
<div id="details" class="section level1">
<h1>Details</h1>
<ul>
<li>Who did you collaborate with: team members of Study Group A14</li>
<li>Approximately how much time did you spend on this problem set: 7 hours on average for each team member</li>
<li>What, if anything, gave you the most trouble: figuring out the correlation with a categorical variable</li>
</ul>
<blockquote>
<p>As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?</p>
</blockquote>
<blockquote>
<p>Yes</p>
</blockquote>
</div>
<div id="rubric" class="section level1">
<h1>Rubric</h1>
<p>Check minus (1/5): Displays minimal effort. Doesn’t complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn’t use plots appropriate for the variables being analyzed.</p>
<p>Check (3/5): Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output).</p>
<p>Check plus (5/5): Finished all components of the assignment correctly and addressed both challenges. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you’ve written additional text to describe how you interpret the output.</p>
</div>
